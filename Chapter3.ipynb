{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "Define a string s = 'colorless'. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colourless\n"
     ]
    }
   ],
   "source": [
    "s='colorless'\n",
    "s=s[0:4]+'u'+s[4:]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "We can use the slice notation to remove morphological endings on words. For example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-ality, un-do, pre-heat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish', 'run', 'nation', 'do', 'heat']\n"
     ]
    }
   ],
   "source": [
    "s=['dishes','running','nationality','undo','preheat']\n",
    "s[0]=s[0][:4]\n",
    "s[1]=s[1][:3]\n",
    "s[2]=s[2][:6]\n",
    "s[3]=s[3][2:]\n",
    "s[4]=s[4][3:]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s='hello'\n",
    "s[-7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it is possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "We can specify a \"step\" size for the slice. The following returns every second character within the slice: monty[6:11:2]. It also works in the reverse direction: monty[10:5:-2] Try these for yourself, then experiment with different step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello hlo olleh olh o\n"
     ]
    }
   ],
   "source": [
    "print(s[::1],s[::2],s[::-1],s[::-2],s[::-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "What happens if you ask the interpreter to evaluate monty[::-1]? Explain why this is a reasonable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nohtyP ytnoM'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty='Monty Python'\n",
    "monty[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It reverses the string. When you specify negative step value and don't specify start and end, it starts at the end of the string and ends at the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "Describe the class of strings matched by the following regular expressions. <br>\n",
    "a. [a-zA-Z]+ <br>\n",
    "b. [A-Z][a-z]* <br>\n",
    "c. p[aeiou]{,2}t <br>\n",
    "d. \\d+(\\.\\d+)? <br>\n",
    "e. ([^aeiou][aeiou][^aeiou])* <br>\n",
    "f. \\w+|[^\\w\\s]+ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regexMatch(pattern,string):\n",
    "    if re.search(pattern,string):\n",
    "        return 'Pattern matched';\n",
    "    else:\n",
    "        return 'No pattern match';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a - checks if the string has atleast one upper or lower case alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matched\n",
      "Pattern matched\n",
      "No pattern match\n",
      "No pattern match\n"
     ]
    }
   ],
   "source": [
    "print(regexMatch(r'[a-zA-Z]+','python'))\n",
    "print(regexMatch(r'[a-zA-Z]+','pyTHon'))\n",
    "print(regexMatch(r'[a-zA-Z]+','12389'))\n",
    "print(regexMatch(r'[a-zA-Z]+',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b - checks if the string has one uppercase letter and an optional lower case letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pattern match\n",
      "Pattern matched\n",
      "No pattern match\n",
      "No pattern match\n",
      "Pattern matched\n"
     ]
    }
   ],
   "source": [
    "print(regexMatch(r'[A-Z][a-z]*','python'))\n",
    "print(regexMatch(r'[A-Z][a-z]*','pyTHon'))\n",
    "print(regexMatch(r'[A-Z][a-z]*','12389'))\n",
    "print(regexMatch(r'[A-Z][a-z]*',''))\n",
    "print(regexMatch(r'[A-Z][a-z]*','PYTHON'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c - checks if the string has p,t and has maximum of two lower cased vowels in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matched\n",
      "Pattern matched\n",
      "No pattern match\n",
      "Pattern matched\n",
      "No pattern match\n",
      "Pattern matched\n"
     ]
    }
   ],
   "source": [
    "print(regexMatch(r'p[aeiou]{,2}t','pat'))\n",
    "print(regexMatch(r'p[aeiou]{,2}t','pout'))\n",
    "print(regexMatch(r'p[aeiou]{,2}t','pouut'))\n",
    "print(regexMatch(r'p[aeiou]{,2}t','teapotted'))\n",
    "print(regexMatch(r'p[aeiou]{,2}t','PAT'))\n",
    "print(regexMatch(r'p[aeiou]{,2}t','pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d - checks if the string has either an integer or a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matched\n",
      "Pattern matched\n",
      "Pattern matched\n",
      "Pattern matched\n",
      "Pattern matched\n",
      "{2.3}.{45}.....\n",
      "Pattern matched\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(regexMatch(r'\\d+(\\.\\d+)?','2.34'))\n",
    "print(regexMatch(r'\\d+(\\.\\d+)?','2.'))\n",
    "print(regexMatch(r'\\d+(\\.\\d+)?','2'))\n",
    "print(regexMatch(r'\\d+(\\.\\d+)?','2.348'))\n",
    "print(regexMatch(r'\\d+(\\.\\d+)?','2.3.45.....'))\n",
    "nltk.re_show(r'\\d+(\\.\\d+)?','2.3.45.....')\n",
    "print(regexMatch(r'\\d+(\\.\\d+)?','23.4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e - checks if the string has an optional consonant,vowel,consonant pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matched\n",
      "Pattern matched\n",
      "Pattern matched\n",
      "Pattern matched\n",
      "Pattern matched\n",
      "Pattern matched\n",
      "{can}{}t{}\n"
     ]
    }
   ],
   "source": [
    "print(regexMatch(r'([^aeiou][aeiou][^aeiou])*','pat'))\n",
    "print(regexMatch(r'([^aeiou][aeiou][^aeiou])*','eat'))\n",
    "print(regexMatch(r'([^aeiou][aeiou][^aeiou])*','cat'))\n",
    "print(regexMatch(r'([^aeiou][aeiou][^aeiou])*','poet'))\n",
    "print(regexMatch(r'([^aeiou][aeiou][^aeiou])*','2.3.45.....'))\n",
    "print(regexMatch(r'([^aeiou][aeiou][^aeiou])*','ct'))\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*','cant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f - checks if the string has atlease one alphanumeric character or no non alphanumeric character followed by a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matched\n",
      "No pattern match\n",
      "Pattern matched\n",
      "Pattern matched\n",
      "Pattern matched\n",
      "{.} {3}\n"
     ]
    }
   ],
   "source": [
    "print(regexMatch(r'\\w+|[^\\w\\s]+','nan '))\n",
    "print(regexMatch(r'\\w+|[^\\w\\s]+',' '))\n",
    "print(regexMatch(r'\\w+|[^\\w\\s]+','2'))\n",
    "print(regexMatch(r'\\w+|[^\\w\\s]+','2.3.45.....'))\n",
    "print(regexMatch(r'\\w+|[^\\w\\s]+','. 3 '))\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+','. 3 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "Write regular expressions to match the following classes of strings: <br>\n",
    "\n",
    "a. A single determiner (assume that a, an, and the are the only determiners). <br>\n",
    "b. An arithmetic expression using integers, addition, and multiplication, such as 2*3+8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matched\n",
      "n{a}ndini\n",
      "Pattern matched\n",
      "n{an}d{an}ini\n",
      "No pattern match\n",
      "No pattern match\n"
     ]
    }
   ],
   "source": [
    "print(regexMatch(r'(a|an|the)','nandini'))\n",
    "nltk.re_show(r'(a|an|the)','nandini')\n",
    "print(regexMatch(r'(an|a|the)','nanandini'))\n",
    "nltk.re_show(r'(an|a|the)','nandanini')\n",
    "print(regexMatch(r'(an|a|the)','123'))\n",
    "print(regexMatch(r'(an|a|the)','feed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matched\n",
      "{2*3+8}\n",
      "Pattern matched\n",
      "{2*3}\n",
      "No pattern match\n",
      "No pattern match\n",
      "No pattern match\n",
      "Pattern matched\n",
      "Pattern matched\n"
     ]
    }
   ],
   "source": [
    "print(regexMatch(r'\\d+[\\*\\+\\-\\%\\d]+\\d+','2*3+8'))\n",
    "nltk.re_show(r'\\d+[\\*\\+\\-\\%\\d]+\\d+','2*3+8')\n",
    "print(regexMatch(r'\\d+[\\*\\+\\-\\%\\d]+\\d+','2*3'))\n",
    "nltk.re_show(r'\\d+[\\*\\+\\-\\%\\d]+\\d+','2*3')\n",
    "print(regexMatch(r'\\d+[\\*\\+\\-\\%\\d]+\\d+','feed'))\n",
    "print(regexMatch(r'\\d+[\\*\\+\\-\\%\\d]+\\d+','2'))\n",
    "print(regexMatch(r'\\d+[\\*\\+\\-\\%\\d]+\\d+','2*'))\n",
    "print(regexMatch(r'\\d+[\\*\\+\\-\\%\\d]+\\d+','2*3+4+5*7'))\n",
    "print(regexMatch(r'\\d+[\\*\\+\\-\\%\\d]+\\d+','2**3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use from urllib import request and then  request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "def readUrl(url):\n",
    "    html=request.urlopen(url).read().decode('utf8')\n",
    "    raw = BeautifulSoup(html).get_text()\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=readUrl('https://www.azlyrics.com/lyrics/rembrandts/illbethereforyouthemefromfriends.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1229 2611\n"
     ]
    }
   ],
   "source": [
    "print(raw.find(\"So no one told you life was gonna be this way\"),raw.rfind(\"('Cause you're there for me too)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So no one told you life was gonna be this way\n",
      "Your job's a joke, you're broke, your love life's D.O.A.\n",
      "It's like you're always stuck in second gear\n",
      "When it hasn't been your day, your week, your month, or even your year, but\n",
      "\n",
      "I'll be there for you\n",
      "(When the rain starts to pour)\n",
      "I'll be there for you\n",
      "(Like I've been there before)\n",
      "I'll be there for you\n",
      "('Cause you're there for me too)\n",
      "\n",
      "You're still in bed at ten and work began at eight\n",
      "You've burned your breakfast, so far things are going great\n",
      "Your mother warned you there'd be days like these\n",
      "But she didn't tell you when the world has brought you down to your knees that\n",
      "\n",
      "I'll be there for you\n",
      "(When the rain starts to pour)\n",
      "I'll be there for you\n",
      "(Like I've been there before)\n",
      "I'll be there for you\n",
      "('Cause you're there for me too)\n",
      "\n",
      "No one could ever know me\n",
      "No one could ever see me\n",
      "Seems you're the only one who knows what it's like to be me\n",
      "Someone to face the day with, make it through all the rest with\n",
      "Someone I'll always laugh with\n",
      "Even at my worst, I'm best with you, yeah!\n",
      "\n",
      "It's like you're always stuck in second gear\n",
      "When it hasn't been your day, your week, your month, or even your year\n",
      "\n",
      "I'll be there for you\n",
      "(When the rain starts to pour)\n",
      "I'll be there for you\n",
      "(Like I've been there before)\n",
      "I'll be there for you\n",
      "('Cause you're there for me too)\n",
      "\n",
      "I'll be there for you\n",
      "I'll be there for you\n",
      "I'll be there for you\n",
      "('Cause you're there for me too)\n"
     ]
    }
   ],
   "source": [
    "print(raw[1229:2643])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9\n",
    "Save some text into a file corpus.txt. Define a function load(f) that reads from the file named in its sole argument, and returns a string containing the text of the file.<br>\n",
    "\n",
    "a. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag (?x).<br>\n",
    "b. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the previous text from html to the file to tokenize it\n",
    "f=open('corpus.txt','w');\n",
    "f.write(raw);\n",
    "f.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(f):\n",
    "    f=open(f,'r');\n",
    "    raw=f.read();\n",
    "    return raw.strip('\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rembrandts - I'll Be There For You (Theme From Friends) Lyrics | AZLyrics.com\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ArtistName = \"The Rembrandts\";\n",
      "SongName = \"I'll Be There For You (Theme From Friends)\";\n",
      "function submitCorrections(){\n",
      "\tdocument.getElementById('corlyr').submit();\n",
      "\treturn false;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  var _comscore = _comscore || [];\n",
      "  _comscore.push({ c1: \"2\", c2: \"6772046\" });\n",
      "  (function() {\n",
      "    var s = document.createElement(\"script\"), el = document.getElementsByTagName(\"script\")[0]; s.async = true;\n",
      "    s.src = (document.location.protocol == \"https:\" ? \"https://sb\" : \"http://b\") + \".scorecardresearch.com/beacon.js\";\n",
      "    el.parentNode.insertBefore(s, el);\n",
      "  })();\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(function(d, s, id) {\n",
      "  var js, fjs = d.getElementsByTagName(s)[0];\n",
      "  if (d.getElementById(id)) return;\n",
      "  js = d.createElement(s); js.id = id;\n",
      "  js.src = \"//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.3\";\n",
      "  fjs.parentNode.insertBefore(js, fjs);\n",
      "}(document, 'script', 'facebook-jssdk'));\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n",
      "K\n",
      "L\n",
      "M\n",
      "N\n",
      "O\n",
      "P\n",
      "Q\n",
      "R\n",
      "S\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n",
      "X\n",
      "Y\n",
      "Z\n",
      "#\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"I'll Be There For You (Theme From Friends)\" lyrics\n",
      "\n",
      "The Rembrandts Lyrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"I'll Be There For You (Theme From Friends)\"\n",
      "\n",
      "\n",
      "\n",
      "So no one told you life was gonna be this way\n",
      "Your job's a joke, you're broke, your love life's D.O.A.\n",
      "It's like you're always stuck in second gear\n",
      "When it hasn't been your day, your week, your month, or even your year, but\n",
      "\n",
      "I'll be there for you\n",
      "(When the rain starts to pour)\n",
      "I'll be there for you\n",
      "(Like I've been there before)\n",
      "I'll be there for you\n",
      "('Cause you're there for me too)\n",
      "\n",
      "You're still in bed at ten and work began at eight\n",
      "You've burned your breakfast, so far things are going great\n",
      "Your mother warned you there'd be days like these\n",
      "But she didn't tell you when the world has brought you down to your knees that\n",
      "\n",
      "I'll be there for you\n",
      "(When the rain starts to pour)\n",
      "I'll be there for you\n",
      "(Like I've been there before)\n",
      "I'll be there for you\n",
      "('Cause you're there for me too)\n",
      "\n",
      "No one could ever know me\n",
      "No one could ever see me\n",
      "Seems you're the only one who knows what it's like to be me\n",
      "Someone to face the day with, make it through all the rest with\n",
      "Someone I'll always laugh with\n",
      "Even at my worst, I'm best with you, yeah!\n",
      "\n",
      "It's like you're always stuck in second gear\n",
      "When it hasn't been your day, your week, your month, or even your year\n",
      "\n",
      "I'll be there for you\n",
      "(When the rain starts to pour)\n",
      "I'll be there for you\n",
      "(Like I've been there before)\n",
      "I'll be there for you\n",
      "('Cause you're there for me too)\n",
      "\n",
      "I'll be there for you\n",
      "I'll be there for you\n",
      "I'll be there for you\n",
      "('Cause you're there for me too)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if  ( /Android|webOS|iPhone|iPod|iPad|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ) \n",
      "  {\n",
      "     document.write('<div style=\"margin-left: auto; margin-right: auto;\">'+\n",
      "  '<iframe scrolling=\"no\" style=\"border: 0px none; overflow:hidden;\" src=\"//adv.mxmcdn.net/br/t1.0/m_js/e_0/sn_0/l_13208539/su_0/rs_0/tr_3vUCAMKkLUIvbcZylVAjPbLoDvqYlUuamn7iD5WmUgBhkvj-nzNnrVNsEtBPpDPCuys9yKD7N3JxxjnK4RMwN7KKvTlM0FeM7t9C5oz2AxcpC8Bm8KhuMonJuar85zT44GhrrDgYiyiHO8VIcc2teTGhFAipSGnRqFmBzpGnrw59seTjP8MW3pNKU1dwH1tJYMaW3pqkZx-dK7r3LoAKkbqeXQ4mDuTqoyMnOfPcKjJGN_0qRgskwAF226TssBS_eTVix_3aEvRp7JoNKQhn7wbohPysb8y8gtq-NuEzcj-aLibDYuZXwn1SzcSYZffbz2BAapLLiu83p-UhWiOkOBr7SRhRS480DewvwN16Re_QpCBiOFdtuIFPzQ6_10aXFbHF5f4KPoiz-UxpMh754KoVazoC7uho8xlVKa64AZtlK6AY-9CS1VT9aCsnqq1y_1BspWwaSjN3oSMc5emkowm_k0M2_wcuADd2DmuqlXQ/\" width=\"290px\" height=\"50px\"></iframe>'+\n",
      "  '</div>');\n",
      "   }\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Submit Corrections\n",
      "\n",
      "Thanks to Aayush Singhal, Mija for correcting these lyrics.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Writer(s): ALLEE WILLIS, DAVID CRANE, MARTA KAUFFMAN, PHIL SOLEM, DANNY WILDE, MICHAEL JAY SKLOFF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"I'll Be There for You\" is a song recorded by duo \"The Rembrandts\" as the theme song to the sitcom \"Friends\". The song was also released as the first single from the third studio album.\n",
      "\n",
      "\n",
      "The original theme to the sitcom, which is under one minute long, was re-recorded as a three-minute song. After Nashville program director Charlie Quinn looped the original short version into a full-length track and broadcast it on radio, it became so popular that they had to re-record it. According to Phil Sōlem, \"Our record label said we had to finish the song and record it. There was no way to get out of it.\"\n",
      "\n",
      "\n",
      "\n",
      "AZLyrics\n",
      "R\n",
      "The Rembrandts Lyrics\n",
      "\n",
      "\n",
      "\n",
      "album: \"LP\" (1995)\n",
      "End Of The Beginning\n",
      "Easy To Forget\n",
      "My Own Way\n",
      "Don't Hide Your Love\n",
      "Drowning In Your Tears\n",
      "This House Is Not A Home\n",
      "April 29\n",
      "Lovin' Me Insane\n",
      "There Goes Lucy\n",
      "As Long As I Am Breathing\n",
      "Call Me\n",
      "Comin' Home\n",
      "What Will It Take\n",
      "The Other Side Of Night\n",
      "I'll Be There For You (Theme From Friends)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit Lyrics\n",
      "Soundtracks\n",
      "Facebook\n",
      "Contact Us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advertise Here\n",
      "Privacy Policy\n",
      "DMCA Policy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Powered by \n",
      "\n",
      "             The Rembrandts lyrics are property and copyright of their owners. \"I'll Be There For You (Theme From Friends)\" lyrics provided for educational purposes and personal use only.\n",
      "\n",
      "                curdate=new Date();\n",
      "                document.write(\"<strong>Copyright &copy; 2000-\"+curdate.getFullYear()+\" AZLyrics.com<\\/strong>\");\n",
      "             \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cf_page_artist = ArtistName;\n",
      "cf_page_song = SongName;\n",
      "cf_page_genre = \"pop\";\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  var _gaq = _gaq || [];\n",
      "  _gaq.push(['_setAccount', 'UA-4309237-1']);\n",
      "\n",
      "  (function() {\n",
      "    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\n",
      "    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n",
      "    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n",
      "  })();\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "window.jQuery || document.write('<script src=\"//www.azlyrics.com/local/jquery.min.js\"><\\/script>')\n",
      "\n",
      "      $(function () {\n",
      "       if ($('#CssFailCheck').is(':visible') === true) {\n",
      "         $('<link rel=\"stylesheet\" type=\"text/css\" href=\"//www.azlyrics.com/bs/css/bootstrap.min.css\"><link rel=\"stylesheet\" href=\"//www.azlyrics.com/bsaz.css\">').appendTo('head');\n",
      "       }\n",
      "      });\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Text from one of the Amazon's job postings\n",
    "raw=load('corpus.txt')\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern= r\"\"\"\\(*[A-Z]?[a-z]+\\)* (?x)\n",
    "            | (?:[A-Z]\\.)+\n",
    "            | (?:[A-Za-z]+\\'[A-Za-z]+)+\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Rembrandts', \"I'll\", 'Be', 'There', 'For', 'You', '(Theme', 'From', 'Friends)', 'Lyrics', 'Lyrics', 'com', 'Artist', 'Name', 'The', 'Rembrandts', 'Song', 'Name', \"I'll\", 'Be', 'There', 'For', 'You', '(Theme', 'From', 'Friends)', 'function', 'submit', 'Corrections', 'document', 'get', 'Element', 'By', 'Id', 'corlyr', 'submit', 'return', 'false', 'var', 'comscore', 'comscore', 'comscore', 'push', 'c', 'c', '(function', 'var', 's', 'document', 'create', 'Element', 'script', 'el', 'document', 'get', 'Elements', 'By', 'Tag', 'Name', 'script', 's', 'async', 'true', 's', 'src', '(document', 'location', 'protocol', 'https', 'https', 'sb', 'http', 'b', 'scorecardresearch', 'com', 'beacon', 'js', 'el', 'parent', 'Node', 'insert', 'Before', '(s', 'el)', '(function', '(d', 's', 'id)', 'var', 'js', 'fjs', 'd', 'get', 'Elements', 'By', 'Tag', 'Name', '(s)', 'if', '(d', 'get', 'Element', 'By', 'Id', '(id))', 'return', 'js', 'd', 'create', 'Element', '(s)', 'js', 'id', 'id', 'js', 'src', 'connect', 'facebook', 'net', 'en', 'sdk', 'js', 'xfbml', 'version', 'v', 'fjs', 'parent', 'Node', 'insert', 'Before', '(js', 'fjs)', '(document', 'script', 'facebook', 'jssdk', 'Search', \"I'll\", 'Be', 'There', 'For', 'You', '(Theme', 'From', 'Friends)', 'lyrics', 'The', 'Rembrandts', 'Lyrics', \"I'll\", 'Be', 'There', 'For', 'You', '(Theme', 'From', 'Friends)', 'So', 'no', 'one', 'told', 'you', 'life', 'was', 'gonna', 'be', 'this', 'way', 'Your', 'job', 's', 'a', 'joke', 'you', 're', 'broke', 'your', 'love', 'life', 's', 'D.O.A.', 'It', 's', 'like', 'you', 're', 'always', 'stuck', 'in', 'second', 'gear', 'When', 'it', 'hasn', 't', 'been', 'your', 'day', 'your', 'week', 'your', 'month', 'or', 'even', 'your', 'year', 'but', \"I'll\", 'be', 'there', 'for', 'you', '(When', 'the', 'rain', 'starts', 'to', 'pour)', \"I'll\", 'be', 'there', 'for', 'you', '(Like', \"I've\", 'been', 'there', 'before)', \"I'll\", 'be', 'there', 'for', 'you', 'Cause', 'you', 're', 'there', 'for', 'me', 'too)', 'You', 're', 'still', 'in', 'bed', 'at', 'ten', 'and', 'work', 'began', 'at', 'eight', 'You', 've', 'burned', 'your', 'breakfast', 'so', 'far', 'things', 'are', 'going', 'great', 'Your', 'mother', 'warned', 'you', 'there', 'd', 'be', 'days', 'like', 'these', 'But', 'she', 'didn', 't', 'tell', 'you', 'when', 'the', 'world', 'has', 'brought', 'you', 'down', 'to', 'your', 'knees', 'that', \"I'll\", 'be', 'there', 'for', 'you', '(When', 'the', 'rain', 'starts', 'to', 'pour)', \"I'll\", 'be', 'there', 'for', 'you', '(Like', \"I've\", 'been', 'there', 'before)', \"I'll\", 'be', 'there', 'for', 'you', 'Cause', 'you', 're', 'there', 'for', 'me', 'too)', 'No', 'one', 'could', 'ever', 'know', 'me', 'No', 'one', 'could', 'ever', 'see', 'me', 'Seems', 'you', 're', 'the', 'only', 'one', 'who', 'knows', 'what', 'it', 's', 'like', 'to', 'be', 'me', 'Someone', 'to', 'face', 'the', 'day', 'with', 'make', 'it', 'through', 'all', 'the', 'rest', 'with', 'Someone', \"I'll\", 'always', 'laugh', 'with', 'Even', 'at', 'my', 'worst', \"I'm\", 'best', 'with', 'you', 'yeah', 'It', 's', 'like', 'you', 're', 'always', 'stuck', 'in', 'second', 'gear', 'When', 'it', 'hasn', 't', 'been', 'your', 'day', 'your', 'week', 'your', 'month', 'or', 'even', 'your', 'year', \"I'll\", 'be', 'there', 'for', 'you', '(When', 'the', 'rain', 'starts', 'to', 'pour)', \"I'll\", 'be', 'there', 'for', 'you', '(Like', \"I've\", 'been', 'there', 'before)', \"I'll\", 'be', 'there', 'for', 'you', 'Cause', 'you', 're', 'there', 'for', 'me', 'too)', \"I'll\", 'be', 'there', 'for', 'you', \"I'll\", 'be', 'there', 'for', 'you', \"I'll\", 'be', 'there', 'for', 'you', 'Cause', 'you', 're', 'there', 'for', 'me', 'too)', 'if', 'Android', 'web', 'i', 'Phone', 'i', 'Pod', 'i', 'Pad', 'Black', 'Berry', 'Mobile', 'Opera', 'Mini', 'i', 'test', '(navigator', 'user', 'Agent)', 'document', 'write', 'div', 'style', 'margin', 'left', 'auto', 'margin', 'right', 'auto', 'iframe', 'scrolling', 'no', 'style', 'border', 'px', 'none', 'overflow', 'hidden', 'src', 'adv', 'mxmcdn', 'net', 'br', 't', 'm', 'js', 'e', 'sn', 'l', 'su', 'rs', 'tr', 'v', 'Kk', 'Ivbc', 'Zyl', 'Aj', 'Pb', 'Lo', 'Dvq', 'Yl', 'Uuamn', 'i', 'Wm', 'Ug', 'Bhkvj', 'nz', 'Nnr', 'Ns', 'Et', 'Pp', 'Cuys', 'y', 'Jxxjn', 'Mw', 'Kv', 'Tl', 'Fe', 't', 'oz', 'Axcp', 'Bm', 'Khu', 'Mon', 'Juar', 'z', 'Ghrr', 'Dg', 'Yiyi', 'Icc', 'te', 'Gh', 'Aip', 'Gn', 'Rq', 'Fm', 'Bzp', 'Gnrw', 'se', 'Tj', 'p', 'dw', 't', 'Ma', 'pqk', 'Zx', 'd', 'r', 'Lo', 'Kkbqe', 'm', 'Du', 'Tqoy', 'Mn', 'Of', 'Pc', 'Kj', 'q', 'Rgskw', 'Tss', 'e', 'Vix', 'a', 'Ev', 'Rp', 'Jo', 'Qhn', 'wboh', 'Pysb', 'y', 'gtq', 'Nu', 'Ezcj', 'a', 'Lib', 'Yu', 'Xwn', 'Szc', 'Zffbz', 'Aap', 'Liu', 'p', 'Uh', 'Wi', 'Ok', 'Br', 'Rh', 'Dewvw', 'Re', 'Qp', 'Bi', 'Fdtu', 'Pz', 'a', 'Fb', 'f', 'Poiz', 'Uxp', 'Mh', 'Ko', 'Vazo', 'uho', 'xl', 'Ka', 'Ztl', 'a', 'Csnqq', 'y', 'Bsp', 'Wwa', 'Sj', 'o', 'Mc', 'emkowm', 'k', 'wcu', 'Dd', 'Dmuql', 'width', 'px', 'height', 'px', 'iframe', 'div', 'Submit', 'Corrections', 'Thanks', 'to', 'Aayush', 'Singhal', 'Mija', 'for', 'correcting', 'these', 'lyrics', 'Writer', '(s)', \"I'll\", 'Be', 'There', 'for', 'You', 'is', 'a', 'song', 'recorded', 'by', 'duo', 'The', 'Rembrandts', 'as', 'the', 'theme', 'song', 'to', 'the', 'sitcom', 'Friends', 'The', 'song', 'was', 'also', 'released', 'as', 'the', 'first', 'single', 'from', 'the', 'third', 'studio', 'album', 'The', 'original', 'theme', 'to', 'the', 'sitcom', 'which', 'is', 'under', 'one', 'minute', 'long', 'was', 're', 'recorded', 'as', 'a', 'three', 'minute', 'song', 'After', 'Nashville', 'program', 'director', 'Charlie', 'Quinn', 'looped', 'the', 'original', 'short', 'version', 'into', 'a', 'full', 'length', 'track', 'and', 'broadcast', 'it', 'on', 'radio', 'it', 'became', 'so', 'popular', 'that', 'they', 'had', 'to', 're', 'record', 'it', 'According', 'to', 'Phil', 'lem', 'Our', 'record', 'label', 'said', 'we', 'had', 'to', 'finish', 'the', 'song', 'and', 'record', 'it', 'There', 'was', 'no', 'way', 'to', 'get', 'out', 'of', 'it', 'Lyrics', 'The', 'Rembrandts', 'Lyrics', 'album', 'End', 'Of', 'The', 'Beginning', 'Easy', 'To', 'Forget', 'My', 'Own', 'Way', 'Don', 't', 'Hide', 'Your', 'Love', 'Drowning', 'In', 'Your', 'Tears', 'This', 'House', 'Is', 'Not', 'Home', 'April', 'Lovin', 'Me', 'Insane', 'There', 'Goes', 'Lucy', 'As', 'Long', 'As', 'Am', 'Breathing', 'Call', 'Me', 'Comin', 'Home', 'What', 'Will', 'It', 'Take', 'The', 'Other', 'Side', 'Of', 'Night', \"I'll\", 'Be', 'There', 'For', 'You', '(Theme', 'From', 'Friends)', 'Search', 'Submit', 'Lyrics', 'Soundtracks', 'Facebook', 'Contact', 'Us', 'Advertise', 'Here', 'Privacy', 'Policy', 'Policy', 'Powered', 'by', 'The', 'Rembrandts', 'lyrics', 'are', 'property', 'and', 'copyright', 'of', 'their', 'owners', \"I'll\", 'Be', 'There', 'For', 'You', '(Theme', 'From', 'Friends)', 'lyrics', 'provided', 'for', 'educational', 'purposes', 'and', 'personal', 'use', 'only', 'curdate', 'new', 'Date', 'document', 'write', 'strong', 'Copyright', 'copy', 'curdate', 'get', 'Full', 'Year', 'Lyrics', 'com', 'strong', 'cf', 'page', 'artist', 'Artist', 'Name', 'cf', 'page', 'song', 'Song', 'Name', 'cf', 'page', 'genre', 'pop', 'var', 'gaq', 'gaq', 'gaq', 'push', 'set', 'Account', '(function', 'var', 'ga', 'document', 'create', 'Element', 'script', 'ga', 'type', 'text', 'javascript', 'ga', 'async', 'true', 'ga', 'src', 'https', 'document', 'location', 'protocol', 'https', 'ssl', 'http', 'www', 'google', 'analytics', 'com', 'ga', 'js', 'var', 's', 'document', 'get', 'Elements', 'By', 'Tag', 'Name', 'script', 's', 'parent', 'Node', 'insert', 'Before', '(ga', 's)', 'window', 'j', 'Query', 'document', 'write', 'script', 'src', 'www', 'azlyrics', 'com', 'local', 'jquery', 'min', 'js', 'script', '(function', 'if', 'Css', 'Fail', 'Check', 'is', 'visible', 'true)', 'link', 'rel', 'stylesheet', 'type', 'text', 'css', 'href', 'www', 'azlyrics', 'com', 'bs', 'css', 'bootstrap', 'min', 'css', 'link', 'rel', 'stylesheet', 'href', 'www', 'azlyrics', 'com', 'bsaz', 'css', 'append', 'To', 'head']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize(raw,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', '1000', 'dollars']\n",
      "['33.3', 'dollars', 'it', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize('I have a 1000 dollars',r'[A-Za-z]+|\\d+\\.?\\d+'))\n",
    "print(nltk.regexp_tokenize('33.3 dollars it is',r'[A-Za-z]+|\\d+\\.?\\d+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', 'is', '2019-02-16']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize('Today is 2019-02-16',r'[A-Za-z]+|\\d{4}-\\d{2}-\\d{2}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nandini']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.regexp_tokenize('I\\'m Nandini',r'[A-Z][a-z]+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10\n",
    "Rewrite the loop as a list comprehension: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
     ]
    }
   ],
   "source": [
    "result=[(word,len(word)) for word in sent]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11\n",
    "Define a string raw containing a sentence of your own choosing. Now, split raw on some character other than space, such as 's'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk', 'org']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw='nltk.org'\n",
    "raw.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 12\n",
    "Write a for loop to print out the characters of a string, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "l\n",
      "t\n",
      "k\n",
      ".\n",
      "o\n",
      "r\n",
      "g\n"
     ]
    }
   ],
   "source": [
    "for c in raw:\n",
    "    print(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13\n",
    "What is the difference between calling split on a string with no argument or with ' ' as the argument, e.g. sent.split() versus sent.split(' ')? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to enter a tab character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog gave John the newspaper'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=' '.join(sent)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent='The\\tdog\\tgave\\tJohn\\tthe\\tnewspaper'\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent='The\\t dog\\t gave\\t John\\t the\\tnewspaper'\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The\\t', 'dog\\t', 'gave\\t', 'John\\t', 'the\\tnewspaper']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent.split(['\\t',' ']) - gives error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent='The  dog  gave  John  the  newspaper'\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', '', 'dog', '', 'gave', '', 'John', '', 'the', '', 'newspaper']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split() identifies all the white space characters and treats them the same (as one) where as split(' ') takes only ' ' as an input and doesn't identify \\t and treats the sequence of spaces as seperate characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14\n",
    "Create a variable words containing a list of words. Experiment with words.sort() and sorted(words). What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'a', 'beautiful', 'night,', \"we're\", 'looking', 'for', 'something', 'dumb', 'to', 'do']\n"
     ]
    }
   ],
   "source": [
    "words='It\\'s a beautiful night, we\\'re looking for something dumb to do'\n",
    "words=words.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'a', 'beautiful', 'do', 'dumb', 'for', 'looking', 'night,', 'something', 'to', \"we're\"]\n"
     ]
    }
   ],
   "source": [
    "words.sort()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'a', 'beautiful', 'night,', \"we're\", 'looking', 'for', 'something', 'dumb', 'to', 'do']\n"
     ]
    }
   ],
   "source": [
    "words='It\\'s a beautiful night, we\\'re looking for something dumb to do'\n",
    "words=words.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'a', 'beautiful', 'do', 'dumb', 'for', 'looking', 'night,', 'something', 'to', \"we're\"]\n",
      "[\"It's\", 'a', 'beautiful', 'night,', \"we're\", 'looking', 'for', 'something', 'dumb', 'to', 'do']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list.sort() changes the underlying list to sorted list and sorted() doesn't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15\n",
    "Explore the difference between strings and integers by typing the following at a Python prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers using int(\"3\") and str(3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3333333 21\n"
     ]
    }
   ],
   "source": [
    "print(\"3\"*7,3*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n"
     ]
    }
   ],
   "source": [
    "print(int(\"3\"),str(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 16\n",
    "Use a text editor to create a file called prog.py containing the single line monty = 'Monty Python'. Next, start up a new session with the Python interpreter, and enter the expression monty at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the .py part of the filename):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prog import monty\n",
    "monty "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, Python should return with a value. You can also try import prog, in which case Python should be able to evaluate the expression prog.monty at the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import prog\n",
    "prog.monty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 17\n",
    "What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Data Scientist' is the sexiest job of the 21st century\n"
     ]
    }
   ],
   "source": [
    "print('%s is the sexiest job of the 21st century'%(\"'Data Scientist'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'IT' is the sexiest job of the 21st century\n"
     ]
    }
   ],
   "source": [
    "print('%6s is the sexiest job of the 21st century'%(\"'IT'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Data Scientist' is the sexiest job of the 21st century\n"
     ]
    }
   ],
   "source": [
    "print('%6s is the sexiest job of the 21st century'%(\"'Data Scientist'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Data Scientist' is the sexiest job of the 21st century\n"
     ]
    }
   ],
   "source": [
    "print('%-6s is the sexiest job of the 21st century'%(\"'Data Scientist'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345678 is the sexiest job of the 21st century\n"
     ]
    }
   ],
   "source": [
    "print('%6s is the sexiest job of the 21st century'%(\"12345678\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345678 is the sexiest job of the 21st century\n"
     ]
    }
   ],
   "source": [
    "print('%-6s is the sexiest job of the 21st century'%(\"12345678\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing happens! If you want to limit the length of the string, just use the below. (+,- are old style to justify the strings right or left respectively. >,< are the new ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Data  is the sexiest job of the 21st century\n"
     ]
    }
   ],
   "source": [
    "print('%.6s is the sexiest job of the 21st century'%(\"'Data Scientist'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 18\n",
    "Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "text=gutenberg.raw('carroll-alice.txt')\n",
    "words=nltk.word_tokenize(text)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WHAT', 'What', 'When', 'Where', 'Which', 'Who', 'Why', 'what', 'when', 'where', 'which', 'who', 'whom', 'whose', 'why']\n"
     ]
    }
   ],
   "source": [
    "whwords=['what','who','which','when','where','why','whose','whom']\n",
    "print(sorted(set([word for word in words if word.lower() in whwords])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there are duplications due to case distinctions. Converting the whole text to lower case avoids this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 19\n",
    "Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using  open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['love', 333333], ['hate', 1233], ['crime', 65]]\n"
     ]
    }
   ],
   "source": [
    "f=open('wordfreq.txt')\n",
    "result=[]\n",
    "for l in f.readlines():\n",
    "    line=l.split();\n",
    "    line[1]=int(line[1]);\n",
    "    result.append(line);\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 20\n",
    "Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural Language Toolkit — NLTK 3.4 documentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK 3.4 documentation\n",
      "\n",
      "next |\n",
      "          modules |\n",
      "          index\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural Language Toolkit¶\n",
      "NLTK is a leading platform for building Python programs to work with human language data.\n",
      "It provides easy-to-use interfaces to over 50 corpora and lexical\n",
      "resources such as WordNet,\n",
      "along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\n",
      "wrappers for industrial-strength NLP libraries,\n",
      "and an active discussion forum.\n",
      "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\n",
      "NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\n",
      "NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\n",
      "NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\n",
      "and “an amazing library to play with natural language.”\n",
      "Natural Language Processing with Python provides a practical\n",
      "introduction to programming for language processing.\n",
      "Written by the creators of NLTK, it guides the reader through the fundamentals\n",
      "of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\n",
      "and more.\n",
      "The online version of the book has been been updated for Python 3 and NLTK 3.\n",
      "(The original Python 2 version is still available at http://nltk.org/book_1ed.)\n",
      "\n",
      "Some simple things you can do with NLTK¶\n",
      "Tokenize and tag some text:\n",
      ">>> import nltk\n",
      ">>> sentence = \"\"\"At eight o'clock on Thursday morning\n",
      "... Arthur didn't feel very good.\"\"\"\n",
      ">>> tokens = nltk.word_tokenize(sentence)\n",
      ">>> tokens\n",
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning',\n",
      "'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      ">>> tagged = nltk.pos_tag(tokens)\n",
      ">>> tagged[0:6]\n",
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'),\n",
      "('Thursday', 'NNP'), ('morning', 'NN')]\n",
      "\n",
      "\n",
      "Identify named entities:\n",
      ">>> entities = nltk.chunk.ne_chunk(tagged)\n",
      ">>> entities\n",
      "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'),\n",
      "           ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'),\n",
      "       Tree('PERSON', [('Arthur', 'NNP')]),\n",
      "           ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'),\n",
      "           ('very', 'RB'), ('good', 'JJ'), ('.', '.')])\n",
      "\n",
      "\n",
      "Display a parse tree:\n",
      ">>> from nltk.corpus import treebank\n",
      ">>> t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
      ">>> t.draw()\n",
      "\n",
      "\n",
      "\n",
      "NB. If you publish work that uses NLTK, please cite the NLTK book as\n",
      "follows:\n",
      "\n",
      "Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O’Reilly Media Inc.\n",
      "\n",
      "\n",
      "Next Steps¶\n",
      "\n",
      "sign up for release announcements\n",
      "join in the discussion\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents¶\n",
      "\n",
      "\n",
      "NLTK News\n",
      "Installing NLTK\n",
      "Installing NLTK Data\n",
      "Contribute to NLTK\n",
      "FAQ\n",
      "Wiki\n",
      "API\n",
      "HOWTO\n",
      "\n",
      "\n",
      "\n",
      "Index\n",
      "Module Index\n",
      "Search Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Table Of Contents\n",
      "\n",
      "NLTK News\n",
      "Installing NLTK\n",
      "Installing NLTK Data\n",
      "Contribute to NLTK\n",
      "FAQ\n",
      "Wiki\n",
      "API\n",
      "HOWTO\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "next |\n",
      "            modules |\n",
      "            index\n",
      "\n",
      "\n",
      "\n",
      "Show Source\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "        © Copyright 2019, NLTK Project.\n",
      "      Last updated on Nov 17, 2018.\n",
      "      Created using Sphinx 1.7.9.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url='http://www.nltk.org/'\n",
    "raw=request.urlopen(url).read().decode('utf8')\n",
    "text=BeautifulSoup(raw).get_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 21\n",
    "Write a function unknown() that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using  re.findall()) and remove any items from this set that occur in the Words Corpus (nltk.corpus.words). Try to categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "def unknown(url):\n",
    "    raw=request.urlopen(url).read().decode('utf8');\n",
    "    text=BeautifulSoup(raw).get_text();\n",
    "    wordsText=re.findall(r'[A-Z]*[a-z]+',text)\n",
    "    englishWords=words.words();\n",
    "    return [w for w in wordsText if (w.lower() not in englishWords) and (w not in englishWords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Affiliated', 'blog', 'https', 'processing', 'Info', 'Stanford', 'openings', 'includes', 'Pageview', 'http', 'Processing', 'gaq', 'applications', 'Directions', 'software', 'toolkit', 'approaches', 'humanities', 'ranges', 'models', 'publications', 'www', 'integrated', 'Github', 'Gates', 'javadocs', 'scenes', 'tagging', 'async', 'ssl', 'Speakers', 'has', 'programmers', 'machines', 'postdocs', 'resulted', 'algorithms', 'areas', 'src', 'parsing', 'technologies', 'sciences', 'js', 'named', 'Elements', 'computers', 'jquery', 'coreference', 'languages', 'Groups', 'var', 'postdoc', 'ul', 'covers', 'javascript', 'nav', 'com', 'members', 'pathname', 'href', 'google', 'answering', 'students'}\n"
     ]
    }
   ],
   "source": [
    "url='https://nlp.stanford.edu/'\n",
    "unknownWords=unknown(url)\n",
    "print(set(unknownWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words in the page that are not in the word corpus are javascript commands, word forms and entity names such as Stanford,Github etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 22\n",
    "Examine the results of processing the URL http://news.bbc.co.uk/ using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly Javascript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222016 227921\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "url='http://news.bbc.co.uk/'\n",
    "raw=request.urlopen(url).read().decode('utf8');\n",
    "text=BeautifulSoup(raw).get_text()\n",
    "print(text.find('BBC News Home'),text.rfind('Find us here'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C News Home\n",
      "BreakingBreaking newsClose breaking news\n",
      "Latest StoriesMost ReadSkip to most readLatest StoriesMost Read\n",
      "\n",
      "\n",
      "\n",
      "Top StoriesTrump: Allies should take back IS fightersThe US president urges European allies to repatriate 800 Islamic State fighters in Iraq and Syria.1han hour agoMiddle EastTrump: Allies should take back IS fightersThe US president urges European allies to repatriate 800 Islamic State fighters in Iraq and Syria.1han hour agoMiddle EastRelated contentControversy over IS returneesDeba\n"
     ]
    }
   ],
   "source": [
    "selectedText=text[222018:227940]\n",
    "print(selectedText[:507])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['News', 'Home', 'Breaking', 'Breaking', 'news', 'Close', 'breaking', 'news', 'Latest', 'Stories', 'Most', 'Read', 'Skip', 'to', 'most', 'read', 'Latest', 'Stories', 'Most', 'Read', 'Top', 'Stories', 'Trump', 'Allies', 'should', 'take', 'back', 'fighters', 'The', 'president', 'urges', 'European', 'allies', 'to', 'repatriate', 'Islamic', 'State', 'fighters', 'in', 'Iraq', 'and', 'Syria', 'han', 'hour', 'ago', 'Middle', 'East', 'Trump', 'Allies', 'should', 'take', 'back', 'fighters', 'The', 'president', 'urges', 'European', 'allies', 'to', 'repatriate', 'Islamic', 'State', 'fighters', 'in', 'Iraq', 'and', 'Syria', 'han', 'hour', 'ago', 'Middle', 'East', 'Related', 'content', 'Controversy', 'over', 'returnees', 'Debate', 'over', 'London', 'teenager', 'Trump', 'Imminent', 'rout', 'of', 'teenager', 'Shamima', 'Begum', 'gives', \"birth'Her\", 'family', 'say', 'they', 'have', 'been', 'told', 'the', 'year', 'old', 'and', 'her', 'child', 'said', 'to', 'be', 'a', 'boy', 'are', 'in', 'good', 'health', 'ma', 'minute', 'ago', 'UKMacron', 'slams', 'yellow', 'vest', 'anti', 'Semitism', 'group', 'of', 'protesters', 'in', 'Paris', 'directed', 'insults', 'and', 'anti', 'Jewish', 'taunts', 'at', 'a', 'prominent', 'philosopher', 'h', 'minutes', 'ago', 'Europe', 'Video', 'minutes', 'seconds', 'Video', 'minutes', 'seconds', 'Video', 'minutes', \"seconds'My\", 'brother', 'the', 'Parkland', 'gunman', 'The', 'whole', 'world', 'is', 'going', 'to', 'give', 'him', 'hate', 'why', 'do', 'says', 'Zachary', 'Cruz', 'of', 'the', 'Parkland', 'shooting', 'gunman', 'h', 'hours', 'ago', 'Canada', 'Why', 'The', 'Night', 'Watch', 'is', 'still', 'full', 'of', 'mysteries', 'Russian', 'flag', 'flown', 'on', 'Salisbury', 'Cathedral', 'The', 'stunt', 'has', 'been', 'called', 'disrespectful', 'and', 'stupid', 'in', 'light', 'of', 'last', \"year's\", 'Novichok', 'poisoning', 'm', 'minutes', 'ago', 'Wiltshire', 'Ex', 'Fox', 'star', 'withdraws', 'bid', 'for', 'role', 'Heather', 'Nauert', 'President', \"Trump's\", 'pick', 'to', 'be', \"America's\", 'new', 'ambassador', 'cites', 'family', 'interests', 'h', 'hours', 'ago', 'Canada', 'Guaid', 'sets', 'up', 'Venezuela', 'aid', 'confrontation', 'The', 'self', 'declared', 'interim', 'leader', 'calls', 'for', 'a', 'humanitarian', 'avalanche', 'on', 'February', 'h', 'hours', 'ago', 'Latin', 'America', 'Caribbean', 'Smollett', 'had', 'no', 'role', 'in', 'own', \"attack'The\", 'denial', 'by', 'the', \"actor's\", 'lawyers', 'comes', 'amid', 'allegations', 'he', 'may', 'have', 'staged', 'the', 'assault', 'han', 'hour', 'ago', 'Canada', 'urges', 'Tory', 'MPs', 'to', 'unite', 'on', 'Brexit', 'Theresa', 'May', 'tells', 'Conservatives', 'history', 'will', 'judge', 'us', 'all', 'over', 'the', 'handling', 'of', \"Britain's\", 'exit', 'han', 'hour', 'ago', 'Politics', 'Video', 'minute', 'seconds', 'Video', 'minute', 'seconds', 'Video', 'minute', 'seconds', 'Tributes', 'as', 'footballer', 'Sala', 'laid', 'to', 'rest', 'On', 'the', 'day', 'of', 'footballer', 'Emiliano', \"Sala's\", 'funeral', 'a', 'childhood', 'friend', 'and', 'a', 'former', 'coach', 'pay', 'tribute', 'h', 'hours', 'ago', 'World', 'urges', 'Tory', 'MPs', 'to', 'unite', 'on', 'Brexit', 'Theresa', 'May', 'tells', 'Conservatives', 'history', 'will', 'judge', 'us', 'all', 'over', 'the', 'handling', 'of', \"Britain's\", 'exit', 'han', 'hour', 'ago', 'Politics', 'Video', 'minute', 'seconds', 'Video', 'minute', 'seconds', 'Video', 'minute', 'seconds', 'Tributes', 'as', 'footballer', 'Sala', 'laid', 'to', 'rest', 'On', 'the', 'day', 'of', 'footballer', 'Emiliano', \"Sala's\", 'funeral', 'a', 'childhood', 'friend', 'and', 'a', 'former', 'coach', 'pay', 'tribute', 'h', 'hours', 'ago', 'World', 'Pakistan', 'rolls', 'out', 'red', 'carpet', 'for', 'Saudi', 'prince', 'Imran', 'Khan', 'needs', 'Saudi', 'money', 'to', 'stave', 'off', 'an', 'economic', 'bail', 'out', 'but', 'this', 'is', 'not', 'a', 'one', 'way', 'relationship', 'h', 'hours', 'ago', 'Asia', 'Jackie', \"Kennedy's\", 'sister', 'Radziwill', 'dies', 'The', 'former', 'first', \"lady's\", 'younger', 'sister', 'who', 'married', 'a', 'prince', 'and', 'became', 'a', 'fashion', 'icon', 'dies', 'at', 'h', 'hours', 'ago', 'Canada', 'Bangladesh', 'slum', 'fire', 'kills', 'nine', 'The', 'blaze', 'in', 'the', 'city', 'of', 'Chittagong', 'may', 'have', 'been', 'sparked', 'by', 'a', 'short', 'circuit', 'officials', 'say', 'h', 'hours', 'ago', 'Asia', 'In', 'Case', 'You', 'Missed', 'It', 'The', 'small', 'town', 'with', 'a', 'dirty', 'secret', 'The', 'Cold', 'War', 'bunkers', 'of', 'Albania', 'Why', 'so', 'many', 'people', 'believe', 'conspiracy', 'theories', 'The', 'fashion', 'models', 'struggling', 'with', 'a', 'life', 'of', 'debt', 'toxic', 'crisis', 'in', 'America', 's', 'coal', 'country', 'Must', 'See', 'Video', 'Video', 'Will', 'quitting', 'Instagram', 'make', 'me', 'happy', 'How', 'to', 'Watch', 'World', 'News', 'TVThe', 'latest', 'global', 'news', 'sport', 'weather', 'and', 'documentaries', 'Audio', 'Listen', 'Live', 'Audio', 'World', 'Service', 'Radio', 'Stories', 'from', 'around', 'the', 'world', 'Near', 'naked', 'Japanese', 'men', 'scramble', 'for', 'lucky', \"sticks'VideoVideoWhat\", 's', 'it', 'like', 'to', 'dress', 'Kendall', 'Jenner', \"VideoVideoBollywood's\", 'first', 'major', 'film', 'Video', 'Video', 'The', 'aerial', 'acrobats', 'who', \"can't\", 'see', 'the', 'ground', 'Video', 'Video', 'Why', 'dedicate', 'my', 'life', 'to', 'monkeys', 'survival', 'Most', 'watched', \"Video'My\", 'brother', 'the', 'Parkland', 'gunman', 'Video', 'Starling', 'murmuration', 'caught', 'on', 'film', \"VideoBollywood's\", 'first', 'major', 'film', 'Video', 'Will', 'quitting', 'Instagram', 'make', 'me', 'happy', 'Video', 'Tributes', 'as', 'footballer', 'Sala', 'laid', 'to', 'rest', 'Full', 'Story', 'Will', 'Trump', 'win', 'his', 'legal', 'fight', 'over', 'the', 'wall', 'Has', \"Meghan's\", 'accent', 'changed', 'After', 'Me', 'Too', 'What', 'learnt', 'at', 'a', 'men', 'only', 'retreat', 'The', 'soldier', 'with', 'a', 'secret', 'talent', 'ballet', 'The', 'man', 'who', 'was', 'able', 'to', 'criticise', 'Xi', 'Jinping', 'The', 'cryptocurrency', 'company', 'that', 'lost', 'm', 'Why', 'Ch', 'Ch', 'Chaka', 'Khan', 'gets', 'annoyed', 'by', 'her', 'greeting', 'Long', 'Reads', 'See', 'All', 'Three', 'things', 'that', 'could', 'stop', 'Elizabeth', 'Warren', 'The', 'downfall', 'of', 'one', 'of', 'the', \"world's\", 'most', 'notorious', 'criminals', 'The', 'radicalisation', 'of', 'Safaa', 'Boular', 'How', 'being', 'a', 'student', 'gun', 'control', 'activist', 'took', 'its', 'toll', 'My', 'disabled', 'son', 'the', 'nobleman', 'the', 'philanderer', 'the', 'detective', 'Most', 'read', 'Trump', 'Allies', 'should', 'take', 'back', 'fighters', 'Smollett', 'had', 'no', 'role', 'in', 'own', 'attack', 'Will', 'Trump', 'win', 'his', 'legal', 'fight', 'over', 'the', 'wall', 'teenager', 'Shamima', 'Begum', 'gives', 'birth', 'Russian', 'flag', 'flown', 'on', 'Salisbury', 'Cathedral', 'Has', \"Meghan's\", 'accent', 'changed', 'Macron', 'slams', 'yellow', 'vest', 'anti', 'Semitism', 'Ex', 'Fox', 'star', 'withdraws', 'bid', 'for', 'role', 'Jackie', \"Kennedy's\", 'sister', 'Radziwill', 'dies', 'Oscars', 'to', 'show', 'awards', 'live', 'after', 'backlash', 'Around', 'the', 'BBCThe', 'dark', 'side', 'of', 'believing', 'in', 'true', 'love', 'Future', 'Is', 'waking', 'up', 'early', 'good', 'or', 'bad', 'Capital', 'How', 'to', 'drink', 'vodka', 'like', 'a', 'Russian', 'Travel', 'Why', 'The', 'Night', 'Watch', 'is', 'still', 'a', 'mystery', 'Culture', 'The', 'puzzle', 'of', 'ancient', 'brain', 'surgery', 'Earth', 'The', 'link', 'between', 'gut', 'health', 'and', 'weight', 'Future', 'bizarre', 'little', 'job', 'writing', 'hashtags', 'Capital', 'Sport', 'See', 'All', 'Live', 'Cup', 'fifth', 'round', 'Wolves', 'withstand', 'Bristol', 'City', 'onslaught', 'watch', 'in', 'play', 'clips', 'Football', 'Live', 'Watch', 'Welsh', 'Open', 'Snooker', 'final', 'Robertson', 'extends', 'lead', 'over', 'Bingham', 'Snooker', 'Live', 'Scottish', 'Premiership', 'Kilmarnock', 'v', 'Celtic', 'build', 'up', 'team', 'news', 'Football', 'Video', 'Video', 'Cavaleiro', 'strike', 'gives', 'Wolves', 'lead', 'h', 'hours', 'ago', 'Football', 'Live', 'Listen', 'Chelsea', 'cruising', 'against', 'Arsenal', 'in', \"women's\", 'Cup', 'fifth', 'round', 'Football', 'Ramos', 'sent', 'off', 'as', 'Real', 'Madrid', 'suffer', 'shock', 'home', 'loss', 'to', 'Girona', 'han', 'hour', 'ago', 'European', 'Football', 'Comments', 'Muir', 'in', 'record', 'squad', 'for', 'Glasgow', 'han', 'hour', 'ago', 'Athletics', 'Find', 'us', 'here', 'News', 'da']\n"
     ]
    }
   ],
   "source": [
    "wordsText=nltk.regexp_tokenize(selectedText,r'[A-Za-z]+\\'[A-Za-z]+|[A-Z]*[a-z]+')\n",
    "print(wordsText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 23\n",
    "Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do and n't? Explain why this regular expression won't work: «n't|\\w+»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't' say no no no no no\""
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=\"Don\\'t' say no no no no no\"\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', 't', 'say', 'no', 'no', 'no', 'no', 'no']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(sent, r'n\\'t|\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'say', 'no', 'no', 'no', 'no', 'no']\n"
     ]
    }
   ],
   "source": [
    "words=re.findall(r\"\\w+(?=n\\'t)|n\\'t|\\w+\",sent)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', \"n't\", 'say', 'no', 'no', 'no', 'no', 'no']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(sent, r'\\w+(?=n\\'t)|n\\'t|\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«n't|\\w+» doesn't work because + is greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 24\n",
    "Try to write code to convert text into hAck3r, using regular expressions and substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map s to two different values: $ for word-initial s, and 5 for word-internal s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertText(text):\n",
    "    encodings={'e':'3','i':'1','o':'0','1':\"|\",'s':'5','\\.':' 5w33t!','ate':'8'}\n",
    "    text=text.lower();\n",
    "    for key, value in encodings.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pur5u|t 0f happyn355'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertText('Pursuit of Happyness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTextEnhanced(text):\n",
    "    encodings={'e':'3','i':'1','o':'0','1':\"|\",'^s|(?<=\\s)s':'$','s':'5','\\.':' 5w33t!','ate':'8'}\n",
    "    text=text.lower();\n",
    "    for key, value in encodings.items():\n",
    "        text = re.sub(key, value, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pur5u|t 0f happyn355'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertTextEnhanced('Pursuit of Happyness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$ur5u|t 0f $appyn355'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertTextEnhanced('Sursuit of sappyness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 25\n",
    "Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string → ingstray, idle → idleay.  http://en.wikipedia.org/wiki/Pig_Latin<br>\n",
    "\n",
    "Write a function to convert a word to Pig Latin.<br>\n",
    "Write code that converts text, instead of individual words.<br>\n",
    "Extend it further to preserve capitalization, to keep qu together (i.e. so that quiet becomes ietquay), and to detect when y is used as a consonant (e.g. yellow) vs a vowel (e.g. style).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPigWord(word):\n",
    "    pig=re.sub(r'^[^AEIOUaeiou]+','',word);\n",
    "    pig+=''.join(re.findall(r'[^'+pig+']',word))\n",
    "    pig+='ay'\n",
    "    return pig;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingstray ingSTRay idleay\n"
     ]
    }
   ],
   "source": [
    "print(getPigWord('string'),getPigWord('STRing'),getPigWord('idle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPigText(text):\n",
    "    if type(text)==str:\n",
    "        text=nltk.word_tokenize(text);\n",
    "    for i,word in enumerate(text):\n",
    "        pig=re.sub(r'^[^AEIOUaeiou]+','',word);\n",
    "        pig+=''.join(re.findall(r'[^'+pig+']',word))\n",
    "        pig+='ay'\n",
    "        text[i]=pig;\n",
    "    text=' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elloHay orldWay'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPigText('Hello World')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 26\n",
    "Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Az', 'Emberi', 'Jogok', 'Egyetemes', 'Nyilatkozata', ...]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import udhr\n",
    "hungary=udhr.words('Hungarian_Magyar-Latin1')\n",
    "hungary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vowelBigrams(text):\n",
    "    text=' '.join(text)\n",
    "    bigrams=nltk.bigrams(re.findall('[AEIOUaeiou]+',text))\n",
    "    return bigrams;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('e', 'e'), ('a', 'a'), ('e', 'o'), ('o', 'i'), ('a', 'ea'), ('ea', 'a'), ('i', 'u'), ('ei', 'i'), ('i', 'i'), ('i', 'A'), ('U', 'a'), ('a', 'ai'), ('ie', 'i'), ('au', 'o'), ('ie', 'u'), ('o', 'A'), ('ie', 'A'), ('e', 'au'), ('a', 'e'), ('e', 'a'), ('i', 'ei'), ('e', 'ea'), ('u', 'e'), ('A', 'E'), ('o', 'E'), ('o', 'ei'), ('o', 'ie'), ('ai', 'a'), ('i', 'o'), ('ai', 'o'), ('e', 'ii'), ('u', 'o'), ('i', 'a'), ('e', 'ei'), ('e', 'ie'), ('u', 'i'), ('o', 'o'), ('u', 'u'), ('ii', 'e'), ('a', 'E'), ('o', 'a'), ('a', 'i'), ('ia', 'e'), ('i', 'ai'), ('ie', 'e'), ('ei', 'e'), ('ai', 'ie'), ('A', 'e'), ('o', 'e'), ('e', 'A'), ('ia', 'i'), ('a', 'u'), ('i', 'e'), ('a', 'ei'), ('o', 'ia'), ('a', 'A'), ('A', 'a'), ('a', 'ia'), ('e', 'i'), ('ei', 'a'), ('E', 'e'), ('ia', 'a'), ('a', 'o'), ('e', 'u'), ('ai', 'e'), ('i', 'E'), ('e', 'ai'), ('e', 'ee'), ('e', 'U'), ('e', 'ia'), ('ee', 'e'), ('u', 'ai'), ('u', 'a'), ('o', 'ai'), ('o', 'u')}\n"
     ]
    }
   ],
   "source": [
    "print(set(vowelBigrams(hungary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "a 336 147  72 142  15 \n",
      "e 133 540 144  71   9 \n",
      "i  66 146  50  40   2 \n",
      "o 151  56  41  51   2 \n",
      "u  19   3   2   4   1 \n"
     ]
    }
   ],
   "source": [
    "bigrams=vowelBigrams(hungary)\n",
    "cfd=nltk.ConditionalFreqDist(bigrams)\n",
    "cfd.tabulate(conditions=['a','e','i','o','u'],samples=['a','e','i','o','u'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 27\n",
    "Python's random module includes a function choice() which randomly chooses an item from a sequence, e.g. choice(\"aehh \") will produce one of four possible characters, with the letter h being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the ''.join() function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: he  haha ee  heheeh eha. Use split() and join() again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def laugh():\n",
    "    raw = ''.join(random.choice('aehh ') for i in range(500))\n",
    "    return ' '.join(raw.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hhhea hheeeea ehhh hhahhh hhhh aah h e ahaaae he h ahheahhhhhhh h eh hhaa hhhheahe haaehhhhehh hehhhhaeeh hhhahhhahh hahhaehha hahee ea hha hhhheea haheaehhehhhh ee ha h ha hehaehahehh hhehhhahee heaah hhaha heahahaa ee hhhehaeehehhh ahh eahhhahhe heahehhh eh h aheaae hhaeha heea ahhhhhhee hh heehaeeha ehh hhhahhheahh hhehaheeehaahhhe hh aaehh haeaah e eh haahahhahhaeeehh ha aaa eaah haae eha ha ehah h hhahaha eehhaeea h heeha aha e hehhaahhah e hha h hhehhha hhh eha aheehhhee'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laugh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 28\n",
    "Consider the numeric expressions in the following sentence from the MedLine Corpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53 +/- 0.15% is three words? Or should we say that it's a single compound word? Or should we say that it is actually nine words, since it's read \"four point five three, plus or minus zero point fifteen percent\"? Or should we say that it's not a \"real\" word at all, since it wouldn't appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One compound word -> would make sense semantically, may be relevant for natural language understanding applications<br>\n",
    "Nine words -> would make sense phonetically, relevant for speech processing applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 29\n",
    "Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for various sections of the Brown Corpus, including section f (lore) and j (learned). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "def getARI(corpus):\n",
    "    ARI=[]\n",
    "    for c in corpus.categories():\n",
    "        u=len(''.join(brown.words(categories=c)))/len(brown.words(categories=c))\n",
    "        s=len(brown.words(categories=c))/len(brown.sents(categories=c));\n",
    "        ARI.append((c,round(4.71*u+0.5*s-21.43,2)))\n",
    "    return ARI;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adventure', 4.08),\n",
       " ('belles_lettres', 10.99),\n",
       " ('editorial', 9.47),\n",
       " ('fiction', 4.91),\n",
       " ('government', 12.08),\n",
       " ('hobbies', 8.92),\n",
       " ('humor', 7.89),\n",
       " ('learned', 11.93),\n",
       " ('lore', 10.25),\n",
       " ('mystery', 3.83),\n",
       " ('news', 10.18),\n",
       " ('religion', 10.2),\n",
       " ('reviews', 10.77),\n",
       " ('romance', 4.35),\n",
       " ('science_fiction', 4.98)]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getARI(brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 30\n",
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "from nltk import LancasterStemmer\n",
    "porter=PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'I', 'see', 'him', 'now', '.', 'He', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.', 'He', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.', '\"', 'While', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',']\n"
     ]
    }
   ],
   "source": [
    "print(text1[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'mobi', 'dick', 'by', 'herman', 'melvil', '1851', ']', 'etymolog', '.', '(', 'suppli', 'by', 'a', 'late', 'consumpt', 'usher', 'to', 'a', 'grammar', 'school', ')', 'the', 'pale', 'usher', '--', 'threadbar', 'in', 'coat', ',', 'heart', ',', 'bodi', ',', 'and', 'brain', ';', 'I', 'see', 'him', 'now', '.', 'He', 'wa', 'ever', 'dust', 'hi', 'old', 'lexicon', 'and', 'grammar', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingli', 'embellish', 'with', 'all', 'the', 'gay', 'flag', 'of', 'all', 'the', 'known', 'nation', 'of', 'the', 'world', '.', 'He', 'love', 'to', 'dust', 'hi', 'old', 'grammar', ';', 'it', 'somehow', 'mildli', 'remind', 'him', 'of', 'hi', 'mortal', '.', '\"', 'while', 'you', 'take', 'in', 'hand', 'to', 'school', 'other', ',']\n"
     ]
    }
   ],
   "source": [
    "print([porter.stem(t) for t in text1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'moby', 'dick', 'by', 'herm', 'melvil', '1851', ']', 'etymolog', '.', '(', 'supply', 'by', 'a', 'lat', 'consum', 'ush', 'to', 'a', 'gramm', 'school', ')', 'the', 'pal', 'ush', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.', 'he', 'was', 'ev', 'dust', 'his', 'old', 'lexicon', 'and', 'gramm', ',', 'with', 'a', 'que', 'handkerchief', ',', 'mock', 'embel', 'with', 'al', 'the', 'gay', 'flag', 'of', 'al', 'the', 'known', 'nat', 'of', 'the', 'world', '.', 'he', 'lov', 'to', 'dust', 'his', 'old', 'gramm', ';', 'it', 'somehow', 'mild', 'remind', 'him', 'of', 'his', 'mort', '.', '\"', 'whil', 'you', 'tak', 'in', 'hand', 'to', 'school', 'oth', ',']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in text1][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "porter stemmer looks better for text1 than lancaster. Lancaster stemmed (for ex) the words like late and consumptive to lat and consum which is not the case with porter stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 31\n",
    " Define the variable saying to contain the list ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']. Process this list using a for loop, and store the length of each word in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list. Now do the same thing using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
    "'is', 'said', 'than', 'done', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "lengths=[]\n",
    "for w in sent:\n",
    "    lengths.append(len(w));\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 3, 4, 1, 4, 2, 4, 4, 4, 1]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(w) for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a variable silly to contain the string: 'newly formed bland ideas are inexpressible in an infuriating way'. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky's famous nonsense phrase, colorless green ideas sleep furiously according to Wikipedia). Now write code to perform the following tasks:<br>\n",
    "\n",
    "Split silly into a list of strings, one per word, using Python's split() operation, and save this to a variable called bland.<br>\n",
    "Extract the second letter of each word in silly and join them into a string, to get 'eoldrnnnna'.<br>\n",
    "Combine the words in bland back into a single string, using join(). Make sure the words in the resulting string are separated with whitespace.<br>\n",
    "Print the words of silly in alphabetical order, one per line.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newly formed bland ideas are inexpressible in an infuriating way\n"
     ]
    }
   ],
   "source": [
    "silly='newly formed bland ideas are inexpressible in an infuriating way'\n",
    "print(silly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "bland=silly.split()\n",
    "print(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eoldrnnnna'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([w[1] for w in bland])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(bland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'are',\n",
       " 'bland',\n",
       " 'formed',\n",
       " 'ideas',\n",
       " 'in',\n",
       " 'inexpressible',\n",
       " 'infuriating',\n",
       " 'newly',\n",
       " 'way']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(silly.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index() function can be used to look up items in sequences. For example, 'inexpressible'.index('e') tells us the index of the first position of the letter e.<br>\n",
    "\n",
    "What happens when you look up a substring, e.g. 'inexpressible'.index('re')?<br>\n",
    "Define a variable words containing a list of words. Now use words.index() to look up the position of an individual word.<br>\n",
    "Define a variable silly as in the exercise above. Use the index() function in combination with list slicing to build a list phrase consisting of all the words up to (but not including) in in silly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'inexpressible'.index('re')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives the starting index of the substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every', 'person', 'is', 'as', 'beautiful', 'as', 'you', 'allow', 'them', 'to', 'be']\n"
     ]
    }
   ],
   "source": [
    "words=['Every','person','is','as','beautiful','as','you','allow','them','to','be']\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.index('person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newly formed bland ideas are inexpressible in an infuriating way'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silly.index('in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "silly=silly.split()\n",
    "print(silly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'an', 'infuriating', 'way']\n"
     ]
    }
   ],
   "source": [
    "print(silly[:silly.index('in')]+silly[silly.index('in')+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 34\n",
    "Write code to convert nationality adjectives like Canadian and Australian to their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 35\n",
    "Read the LanguageLog post on phrases of the form as best as p can and as best p can, where p is a pronoun. Investigate this phenomenon with the help of a corpus and the findall() method for searching tokenized text described in 3.5. http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feed': {'language': 'en-US',\n",
       "  'title': 'Language Log',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': 'en-US',\n",
       "   'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "   'value': 'Language Log'},\n",
       "  'subtitle': '',\n",
       "  'subtitle_detail': {'type': 'text/plain',\n",
       "   'language': 'en-US',\n",
       "   'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "   'value': ''},\n",
       "  'updated': '2019-02-17T15:03:09Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=17, tm_hour=15, tm_min=3, tm_sec=9, tm_wday=6, tm_yday=48, tm_isdst=0),\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'http://languagelog.ldc.upenn.edu/nll'},\n",
       "   {'rel': 'self',\n",
       "    'type': 'application/atom+xml',\n",
       "    'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom'}],\n",
       "  'link': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "  'id': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom',\n",
       "  'guidislink': False},\n",
       " 'entries': [{'authors': [{'name': 'Mark Liberman',\n",
       "     'href': 'http://ling.upenn.edu/~myl'}],\n",
       "   'author_detail': {'name': 'Mark Liberman',\n",
       "    'href': 'http://ling.upenn.edu/~myl'},\n",
       "   'href': 'http://ling.upenn.edu/~myl',\n",
       "   'author': 'Mark Liberman',\n",
       "   'title': 'Emergency in B flat',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Emergency in B flat'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41874'},\n",
       "    {'href': 'http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1.wav',\n",
       "     'rel': 'enclosure',\n",
       "     'length': '1341634',\n",
       "     'type': 'audio/wav'},\n",
       "    {'href': 'http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_1.wav',\n",
       "     'rel': 'enclosure',\n",
       "     'length': '87522',\n",
       "     'type': 'audio/wav'},\n",
       "    {'href': 'http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_2.wav',\n",
       "     'rel': 'enclosure',\n",
       "     'length': '50882',\n",
       "     'type': 'audio/wav'},\n",
       "    {'href': 'http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_3.wav',\n",
       "     'rel': 'enclosure',\n",
       "     'length': '74818',\n",
       "     'type': 'audio/wav'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41874#comments',\n",
       "     'count': '0',\n",
       "     'thr:count': '0'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41874',\n",
       "     'count': '0',\n",
       "     'thr:count': '0'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41874',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41874',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-17T15:03:09Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=17, tm_hour=15, tm_min=3, tm_sec=9, tm_wday=6, tm_yday=48, tm_isdst=0),\n",
       "   'published': '2019-02-17T14:12:34Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=17, tm_hour=14, tm_min=12, tm_sec=34, tm_wday=6, tm_yday=48, tm_isdst=0),\n",
       "   'tags': [{'term': 'Intonation',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Rhetoric',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': \"In his 2/15/2019 announcement about declaring a state of emergency on the southern border, President Trump used a sequence of a dozen or so singsong phrases: Your browser does not support the audio element. So the uh the order is signed. And uh I'll f- I'll sign the final papers as soon as I get [&#8230;]\",\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': \"In his 2/15/2019 announcement about declaring a state of emergency on the southern border, President Trump used a sequence of a dozen or so singsong phrases: Your browser does not support the audio element. So the uh the order is signed. And uh I'll f- I'll sign the final papers as soon as I get [&#8230;]\"},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41874',\n",
       "     'value': '<p>In his <a href=\"https://youtu.be/M1k_G0yIqFE?t=1382\" target=\"_blank\" rel=\"noopener\">2/15/2019 announcement</a> about declaring a state of emergency on the southern border, President Trump used a sequence of a dozen or so singsong phrases:</p>\\n<div style=\"padding-left: 30px;\"><audio style=\"width: 230px;\" controls=\"controls\"><source src=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1.wav\" type=\"audio/wav\" />Your browser does not support the audio element.</audio></div>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #800000;\">So the uh the order is signed. And uh I\\'ll f- I\\'ll sign the final papers as soon as I get into the Oval Office. And we will have a national emergency, and we will then be sued, and they will sue us in the 9th Circuit, uh even though it shouldn\\'t be there, and we will possibly get a bad ruling, and then we\\'ll get another bad ruling, and then we\\'ll end up in the Supreme Court, and hopefully we\\'ll get a fair shake and we\\'ll win in the Supreme Court. Just like the ban, they sued us in the 9th Circuit and we lost, and then we lost in the appellate division, and then we went to the Supreme Court and we won.</span></p>\\n<p><span id=\"more-41874\"></span></p>\\n<p>I\\'ve previously described cases where Trump\\xa0 uses singsong intonation or stretches of level pitch in a chanted voice quality. Thus in\\xa0 \"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=27267\" target=\"_blank\" rel=\"noopener\">Trump\\'s prosody</a>\", 8/8/2016, I described \"six\\xa0repetitions — three pairs — of a binary\\xa0pattern, in which a low-pitched region is followed by an abrupt jump to a much higher value, in some cases more than an octave up,\\xa0starting with an emphasized word and continuing nearly level to the end of the phrase\". I suggested that</p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #000080;\">I suspect that there\\'s a bit of New York City here. And I think that Mr. Trump tends to use this pattern\\xa0when he enumerates problems or promises that he thinks ought to be obvious or at least already known.</span></p>\\n<p>And <a href=\"https://mypages.unh.edu/rsburdin/home\" target=\"_blank\" rel=\"noopener\">Rachel Steindel Burdin</a> wrote to say that</p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #800000;\">We did a couple of perceptual experiments, which found first, in a forced choice task, subjects reliably hear these rises-to-plateaus as signaling that speaker thinks the listeners already knows the items in the list, and second, subjects hear the plateaus as making the speaker sound condescending. \\xa0 As for the potential New York connection–I looked into Jewish English intonation for my dissertation [&#8230;], and found some evidence for increased use of these types of contours by older Jews in listing environments. So, I\\'d say it\\'s definitely likely, at least to the extent that we want to connect Jewish English intonation to New York City intonation.</span></p>\\n<p>In this case, Trump starts with a less-obviously chanted version of the pattern:</p>\\n<div style=\"padding-left: 30px;\"><audio style=\"width: 230px;\" controls=\"controls\"><source src=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_1.wav\" type=\"audio/wav\" />Your browser does not support the audio element.</audio></div>\\n<p><a href=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_1.png\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_1.png\" width=\"490\" /></a></p>\\n<p>&#8230;and then goes into a long sequence where the syllable-by-syllable pitches are more level in pitch and sound more like singing, and the phrase-final level stretches are higher in pitch, as in the next couple of phrases:</p>\\n<div style=\"padding-left: 30px;\"><audio style=\"width: 230px;\" controls=\"controls\"><source src=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_2.wav\" type=\"audio/wav\" />Your browser does not support the audio element.</audio></div>\\n<p><a href=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_2.png\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_2.png\" width=\"490\" /></a></p>\\n<div style=\"padding-left: 30px;\"><audio style=\"width: 230px;\" controls=\"controls\"><source src=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_3.wav\" type=\"audio/wav\" />Your browser does not support the audio element.</audio></div>\\n<p><a href=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_3.png\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/myl/TrumpEmergencyX1_3.png\" width=\"490\" /></a></p>\\n<p>&#8230;and so on.</p>\\n<p>Measuring the average f0 of the final level stretches suggests that once again, the president is performing in (or around) the key of B♭:</p>\\n<p><a href=\"http://languagelog.ldc.upenn.edu/myl/EmergencyInBflat.png\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/myl/EmergencyInBflat.png\" width=\"490\" /></a></p>\\n<ol>\\n<li>And we will have a national emergency</li>\\n<li>and we will then be sued</li>\\n<li>and they will sue us in the 9th Circuit</li>\\n<li>uh even though it shouldn\\'t be there</li>\\n<li>and we will possibly get a bad ruling</li>\\n<li>and then we\\'ll get another bad ruling</li>\\n<li>and then we\\'ll end up in the Supreme Court</li>\\n<li>and hopefully we\\'ll get a fair shake</li>\\n<li>and we\\'ll win in the Supreme Court</li>\\n<li>Just like the ban</li>\\n<li>they sued us in the 9th Circuit</li>\\n<li>and we lost</li>\\n<li>and then we lost in the appellate division</li>\\n<li>and then we went to the Supreme Court</li>\\n<li>and we won</li>\\n</ol>\\n<p>Alec Baldwin\\'s SNL version is <a href=\"https://youtu.be/8vQlhWBvAwY?t=121\" target=\"_blank\" rel=\"noopener\">here</a>\\xa0&#8212; and again <a href=\"https://youtu.be/8vQlhWBvAwY?t=384\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\\n<p>For more on quasi-chanted passages in Trump\\'s rhetoric, see \"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=28610\" target=\"_blank\" rel=\"noopener\">Trumpchant in B flat</a>\", 10/2/2016; \"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=35816\" target=\"_blank\" rel=\"noopener\">Blues in Moore flat</a>\", 12/15/2017.</p>'}],\n",
       "   'thr_total': '0'},\n",
       "  {'authors': [{'name': 'Victor Mair',\n",
       "     'href': 'https://www.sas.upenn.edu/ealc/mair'}],\n",
       "   'author_detail': {'name': 'Victor Mair',\n",
       "    'href': 'https://www.sas.upenn.edu/ealc/mair'},\n",
       "   'href': 'https://www.sas.upenn.edu/ealc/mair',\n",
       "   'author': 'Victor Mair',\n",
       "   'title': 'Japanese varia',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Japanese varia'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41836'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41836#comments',\n",
       "     'count': '2',\n",
       "     'thr:count': '2'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41836',\n",
       "     'count': '2',\n",
       "     'thr:count': '2'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41836',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41836',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-17T03:05:14Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=17, tm_hour=3, tm_min=5, tm_sec=14, tm_wday=6, tm_yday=48, tm_isdst=0),\n",
       "   'published': '2019-02-17T03:04:22Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=17, tm_hour=3, tm_min=4, tm_sec=22, tm_wday=6, tm_yday=48, tm_isdst=0),\n",
       "   'tags': [{'term': 'Language and culture',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Language and food',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Language and technology',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Onomatopoeia',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'First, as a slightly belated Valentine\\'s present, onomatopoetic / mimetic chocolates: \"Chocolates That Represent Japanese Onomatopoeic Words To Describe Texture\", by Johnny, Spoon &#38; Tamago (1/16/15) Here are the names of eight of the nine chocolates designed by Oki Sato of the Tokyo and Milan-based design studio Nendo: ツブツブ (tsubu tsubu): a word for small [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'First, as a slightly belated Valentine\\'s present, onomatopoetic / mimetic chocolates: \"Chocolates That Represent Japanese Onomatopoeic Words To Describe Texture\", by Johnny, Spoon &#38; Tamago (1/16/15) Here are the names of eight of the nine chocolates designed by Oki Sato of the Tokyo and Milan-based design studio Nendo: ツブツブ (tsubu tsubu): a word for small [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41836',\n",
       "     'value': '<p>First, as a slightly belated Valentine\\'s present, onomatopoetic / mimetic chocolates:</p>\\n<p style=\"padding-left: 30px;\">\"<a href=\"http://www.spoon-tamago.com/2015/01/16/nendo-chocolates-japanese-onomatopoeic-words-texture/\">Chocolates That Represent Japanese Onomatopoeic Words To Describe Texture</a>\", by Johnny, Spoon &amp; Tamago (1/16/15)</p>\\n<p>Here are the names of eight of the nine chocolates designed by Oki Sato of the Tokyo and Milan-based design studio Nendo:</p>\\n<p style=\"padding-left: 30px;\">ツブツブ (tsubu tsubu): a word for small bits or drops<br />\\nスベスベ (sube sube): smooth edges and corners<br />\\nトゲトゲ (toge toge): sharp pointed tips<br />\\nザラザラ (zara zara): granular like a file<br />\\nゴロゴロ (goro goro): cubic, with many edges<br />\\nフワフワ (fuwa fuwa): soft and airy with many tiny holes<br />\\nポキポキ (poki poki): a delicate frame or structure<br />\\nザクザク (zaku zaku): makes a crunching sounds, like when you step on ice</p>\\n<p>You can see exceptionally clear photographs of the ingeniously designed 26x26x26mm chocolates in the article linked above.</p>\\n<p>[h.t. Becki Kanou]</p>\\n<p><span id=\"more-41836\"></span></p>\\n<p>From Nathan Hopson:</p>\\n<p style=\"padding-left: 30px;\">Second, funerary capitalism meets tongue-in-cheek wordplay with my new favorite Japanese word/service: スマ墓 (\"smart grave\"). Read <i>sumabo</i>, this new-ish service uses what appears to be roughly the same augmented reality technologies as, say, Pokémon Go so that your phone will show prerecorded images and video of deceased loved ones when the application is activated at the correct GPS coordinates. For just ¥500/month, the service provider not only creates and maintains this virtual (\"smart\") grave, but will also keep for a period of 15 years the cremated ashes that would have gone into a real grave.</p>\\n<p style=\"padding-left: 30px;\">What got me about this is not just that it\\'s a clever entry into the ever-expanding perimortuary business here in Japan, the most aged country in world history, but that the service is almost perfectly homophonous with the Japanese contraction of \"smart phone.\" Japanese mobiles are still called by that name as well (携帯電話 <i>keitai denwa</i>), but with the market turnover from the old flip phones &#8212; cleverly, if self-deprecatingly, referred to as ガラパゴス系・ガラパゴス携 or ガラ系・ガラ携・ガラケー (<i>Garapagosu-kei / Gara-kei</i>) for short because they have followed a unique evolutionary process on \"remote\" islands&#8230; &#8212; almost complete in the past several years, these days one hears <i>sumaho</i> more often. Who knows how often we will hear <i>sumabo</i> in the future?</p>\\n<p style=\"padding-left: 30px;\">Articles with good pictures <a href=\"https://dot.asahi.com/aera/2018080700054.html?page=1\">here</a> and <a href=\"https://jisin.jp/life/living/1595402/\">here</a> (Japanese only).</p>\\n<p>Japanese, like Korean, is exceptionally rich in and fond of onomatopoeia.</p>\\n<p><b>Readings</b></p>\\n<p style=\"padding-left: 30px;\">\"<a title=\"Permanent link to Korean refrigerator onomatopoeia\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=40661\" rel=\"bookmark\">Korean refrigerator onomatopoeia</a>\" (11/10/18)</p>\\n<p style=\"padding-left: 30px;\">\"<a title=\"Permanent link to Japanese (and Chinese) Onomatopoeia\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=387\" rel=\"bookmark\">Japanese (and Chinese) Onomatopoeia</a>\" (7/21/08)</p>\\n<p style=\"padding-left: 30px;\">\"<a title=\"Permanent link to Duang\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=17913\" rel=\"bookmark\">Duang</a>\" (3/1/15)</p>'}],\n",
       "   'thr_total': '2'},\n",
       "  {'authors': [{'name': 'Mark Liberman',\n",
       "     'href': 'http://ling.upenn.edu/~myl'}],\n",
       "   'author_detail': {'name': 'Mark Liberman',\n",
       "    'href': 'http://ling.upenn.edu/~myl'},\n",
       "   'href': 'http://ling.upenn.edu/~myl',\n",
       "   'author': 'Mark Liberman',\n",
       "   'title': '&quot;Speech synthesis&quot;',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': '&quot;Speech synthesis&quot;'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41839'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41839#comments',\n",
       "     'count': '13',\n",
       "     'thr:count': '13'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41839',\n",
       "     'count': '13',\n",
       "     'thr:count': '13'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41839',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41839',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-16T19:30:33Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=16, tm_hour=19, tm_min=30, tm_sec=33, tm_wday=5, tm_yday=47, tm_isdst=0),\n",
       "   'published': '2019-02-16T14:33:16Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=16, tm_hour=14, tm_min=33, tm_sec=16, tm_wday=5, tm_yday=47, tm_isdst=0),\n",
       "   'tags': [{'term': 'Speech technology',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'Ordinary language and technical terminology often diverge. We\\'ve covered the \"passive voice\" case at length. I don\\'t think we\\'ve discussed\\xa0 the fact that for botanists, cucumbers and tomatoes are berries but strawberries and raspberries aren\\'t &#8212; but there are many examples of such terminological divergence in fields outside of linguistics. However, the technical terminology is [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Ordinary language and technical terminology often diverge. We\\'ve covered the \"passive voice\" case at length. I don\\'t think we\\'ve discussed\\xa0 the fact that for botanists, cucumbers and tomatoes are berries but strawberries and raspberries aren\\'t &#8212; but there are many examples of such terminological divergence in fields outside of linguistics. However, the technical terminology is [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41839',\n",
       "     'value': '<p>Ordinary language and technical terminology often diverge. We\\'ve covered the \"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=2922\" target=\"_blank\" rel=\"noopener\">passive voice</a>\" case at length. I don\\'t think we\\'ve discussed\\xa0 the fact that <a href=\"https://en.wikipedia.org/wiki/Berry_(botany)\" target=\"_blank\" rel=\"noopener\">for botanists</a>, cucumbers and tomatoes are berries but strawberries and raspberries aren\\'t &#8212; but there are many examples of such terminological divergence in fields outside of linguistics. However, the technical terminology is itself sometimes vague or ambiguous in ways that lead to confusion among outsiders, and today I want to explore one case of this kind: \"speech synthesis\".</p>\\n<p><span id=\"more-41839\"></span></p>\\n<p>Andrew Liszewski, \"<a href=\"https://gizmodo.com/my-favorite-childhood-gadget-of-the-80s-the-speak-sp-1832140129\" target=\"_blank\" rel=\"noopener\">My Favorite Childhood Gadget of the \\'80s, the Speak &amp; Spell, Is Back</a>\", <em>Gizmodo</em> 2/18/2019:</p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #000080;\">By today’s standards, the Speak &amp; Spell is beyond primitive, but when introduced by Texas Instruments at CES in 1978, it was one of the first handheld devices to incorporate an electronic display, expansion cartridges, and a speech synthesis engine that could say and spell over 200 words. It even ran on one of the first microprocessors, the TMS1000, which was a power hog that would quickly drain the toy’s four C-sized batteries.</span></p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #000080;\">The Speak &amp; Spell’s\\xa0computerized voice\\xa0was its most impressive feature, and it was so fascinating to me as a kid that I can still clearly hear its raspy, slightly incomprehensible pronunciations in my head; when I’d be hard-pressed to remember the voices of any of my childhood friends. [&#8230;]</span></p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #000080;\">Where the new Speak &amp; Spell differs from the original—and this could be a deal-breaker for some nostalgia-seekers—is its voice. <mark>Instead of using a synthesizer that generates spoken words from a bunch of coded instructions, Basic Fun!’s Speak &amp; Spell uses voice recordings that have been processed to sound like they’re being generated by a computer.</mark> The monotonous, stilted delivery sounds very close to the original version, but it’s definitely different.</span></p>\\n<p>The highlighted region is somewhere between confused and false. The original 1978 TI Speak &amp; Spell used naturally-spoken speech that was compressed via LPC (\"linear predictive coding\") so as to fit on the then-available device memory, and then uncompressed for playback using TI\\'s then-new LPC chip. So it\\'s true that it \"generat(ed) spoken words from a bunch of coded instructions\". But so (I assume) does the modern imitation. And so does your cell phone, and your .mp3 or .aac-encoded podcasts, and your Audible audiobooks, etc.</p>\\n<p>It\\'s true that this process of reconstituting compressed or \"encoded\" speech is commonly called \"(re-)synthesis\". But the reason that the original Speak &amp; Spell produced \"raspy, slightly incomprehensible pronunciations\" is partly that it used extreme compression &#8212; about 1200 bits per second, as opposed to 64000 or 128000 bps for typical .mp3 audio, or 4750 to 12200 bps for GSM cellular voice transmission &#8212; and partly that it used an early-generation encoding algorithm.</p>\\n<p>The <a href=\"https://en.wikipedia.org/wiki/Speak_%26_Spell_(toy)\" target=\"_blank\" rel=\"noopener\">Wikipedia entry</a> gives more details but is also misleading:</p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #800000;\">The Speak &amp; Spell used the first single-chip voice synthesizer, the TMC0280, later called the TI TMS5100, which utilized a 10th-order linear predictive coding (LPC) model by using pipelined electronic DSP logic. A variant of this chip with a very similar voice would eventually be utilized in certain Chrysler vehicles in the 1980s as the Electronic Voice Alert.</span></p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #800000;\"><mark>Speech synthesis data (phoneme data) for the spoken words were stored on a pair of 128 Kbit metal gate PMOS ROMs.</mark></span></p>\\n<p>As the Wikipedia article itself goes on to explain, it\\'s not \"phoneme data\" that was stored &#8212; which would generally have been the case if the system had used a text-to-speech algorithm &#8212; but rather the time functions of linear prediction parameters, f0, amplitude, and so on, derived from human recordings of the specific words to be spoken:</p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #800000;\">The technique used to create the words was to have a professional speaker speak the words. The utterances were captured and processed. Originally all of the recording and processing was completed in Dallas. By 1982 when the British, French, Italian and German versions were being developed, the original voices were recorded in the TI facility near Nice in France and these full bit rate digital recordings were sent to Dallas for processing using a minicomputer.[35] Some weeks later the processed data was returned and required significant hand editing to fix the voicing errors which had occurred during the process. The data rate was so radically cut that all of the words needed some editing. In some cases this was fairly simple, but some words were unintelligible and required days of work and others had to be completely scrapped. The stored data were for the specific words and phrases used in the Speak &amp; Spell. The data rate was about 1,000 bits per second.</span></p>\\n<p>For some background on LPC, see\\xa0 Bishnu Atal, \"<a href=\"http://www.uncini.com/dida/aacp/dida_mat/2006_Atal_The%20history%20of%20linear%20prediction.pdf\" target=\"_blank\" rel=\"noopener\">The History of Linear Prediction</a>\", <em>IEEE Signal Processing Magazine</em> 3/2006.\\xa0 Bishnu uses the older and less confusion term \"vocoder\" (= Voice Coder) to refer to the process of compressing and reconstituting speech &#8212; but he also writes about \"LPC analysis and resynthesis\":</p>\\n<p style=\"padding-left: 30px;\">LPC rapidly became a very popular topic in speech research. A large number of people contributed valuable ideas for the application of the basic theory of linear prediction to speech analysis and synthesis. The excitement was evident at practically every technical meeting. Research on LPC vocoders gained momentum partly due to increased funding from the U.S. government and its selection for the 2.4 kb/s secure-voice standard LPC10. LPC required a lot of computations when it started being applied to speech. Fortunately, computer technology was rapidly evolving. By 1973, the first compact real-time LPC vocoder had been implemented at Philco-Ford. In 1978, Texas Instruments introduced a popular LPC-based toy that was called “Speak and Spell.”</p>\\n<p>And Bishnu\\'s seminal 1971 paper had the title \"<a href=\"https://asa.scitation.org/doi/10.1121/1.1912679\" target=\"_blank\" rel=\"noopener\">Speech analysis and synthesis by linear prediction of the speech wave</a>\" &#8212; and uses the word \"synthesizer\" to describe the subsystem for reconstituting the speech signal:</p>\\n<p style=\"padding-left: 30px;\"><strong>Abstract:</strong> We describe a procedure for efficient encoding of the speech wave by representing it in terms of time‐varying parameters related to the transfer function of the vocal tract and the characteristics of the excitation. The speech wave, sampled at 10 kHz, is analyzed by predicting the present speech sample as a linear combination of the 12 previous samples. The 12 predictor coefficients are determined by minimizing the mean‐squared error between the actual and the predicted values of the speech samples. Fifteen parameters—namely, the 12 predictor coefficients, the pitch period, a binary parameter indicating whether the speech is voiced or unvoiced, and the rms value of the speech samples—are derived by analysis of the speech wave, encoded and transmitted to the synthesizer. The speech wave is synthesized as the output of a linear recursive filter excited by either a sequence of quasiperiodic pulses or a white‐noise source. Application of this method for efficient transmission and storage of speech signals as well as procedures for determining other speech characteristics, such as formant frequencies and bandwidths, the spectral envelope, and the autocorrelation function, are discussed.</p>\\n<p>In talking about food or fibers or drugs, we generally don\\'t use the words <em>synthetic, synthesis</em> etc. to describe things created by processing natural substances. Synthetic fabrics are derived from petroleum or whatever, not by processing plant materials. But in the case of speech, <em>synthesis</em> is used to describe reconstituting a stream of natural speech (or other audio) that has been processed for more efficient transmission, as well as to describe the creation of entirely-synthetic audio as in text-to-speech applications. (Though in fairness, TTS these days mostly works by combination and modification of fragments of human speech from a large collection of naturally-spoken audio, or by a fuzzier \"deep learning\" creation of audio time-functions by analogy to a large body of natural training speech &#8212; this is something like fibers created by extraction and polymerization of molecular fragments from an organic source.)</p>\\n<p>&nbsp;</p>\\n<p><iframe width=\"500\" height=\"375\" src=\"https://www.youtube.com/embed/UwWaeEyhPP0?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>'}],\n",
       "   'thr_total': '13'},\n",
       "  {'authors': [{'name': 'Victor Mair',\n",
       "     'href': 'https://www.sas.upenn.edu/ealc/mair'}],\n",
       "   'author_detail': {'name': 'Victor Mair',\n",
       "    'href': 'https://www.sas.upenn.edu/ealc/mair'},\n",
       "   'href': 'https://www.sas.upenn.edu/ealc/mair',\n",
       "   'author': 'Victor Mair',\n",
       "   'title': 'Sinitic for &quot;iron&quot; in Balto-Slavic',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Sinitic for &quot;iron&quot; in Balto-Slavic'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41832'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41832#comments',\n",
       "     'count': '29',\n",
       "     'thr:count': '29'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41832',\n",
       "     'count': '29',\n",
       "     'thr:count': '29'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41832',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41832',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-15T14:04:47Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=15, tm_hour=14, tm_min=4, tm_sec=47, tm_wday=4, tm_yday=46, tm_isdst=0),\n",
       "   'published': '2019-02-15T12:57:01Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=15, tm_hour=12, tm_min=57, tm_sec=1, tm_wday=4, tm_yday=46, tm_isdst=0),\n",
       "   'tags': [{'term': 'Borrowing',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Historical linguistics',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Phonetics and phonology',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': '[This is a guest post by Chris Button] There are a couple of brief suggestions in Mallory &#38; Adams\\' Encyclopedia of Indo-European Culture (1997:314;379) that the Lithuanian word geležis and Old Church Slavonic word želežo for \"iron\", which following Derksen (2008:555) may be derived from Balto-Slavic *geleź-/*gelēź- (ź being the IPA palatal sibilant ʑ), could [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': '[This is a guest post by Chris Button] There are a couple of brief suggestions in Mallory &#38; Adams\\' Encyclopedia of Indo-European Culture (1997:314;379) that the Lithuanian word geležis and Old Church Slavonic word želežo for \"iron\", which following Derksen (2008:555) may be derived from Balto-Slavic *geleź-/*gelēź- (ź being the IPA palatal sibilant ʑ), could [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41832',\n",
       "     'value': '<p>[This is a guest post by Chris Button]</p>\\n<p>There are a couple of brief suggestions in Mallory &amp; Adams\\' <i>Encyclopedia of Indo-European Culture</i> (1997:314;379) that the Lithuanian word geležis and Old Church Slavonic word želežo for \"iron\", which following Derksen (2008:555) may be derived from Balto-Slavic *geleź-/*gelēź- (ź being the IPA palatal sibilant ʑ), could possibly have a Proto-Sino-Tibetan association.</p>\\n<p><span id=\"more-41832\"></span>The Old Chinese word for 鐵 \"iron\" may be reconstructed as *ɬə́c which certainly looks promising in that regard. Since the Balto-Slavic form appears to be an isolate in Indo-European, whereas 鐵 *ɬə́c belongs to an extensive word family connected to shininess, most directly in this case with 錫 *sɬác \"tin\" following a proposal by Schuessler (note the ə/a ablaut), the direction of the putative loan must be into Balto-Slavic rather than into Chinese.</p>\\n<p><strong>Readings</strong></p>\\n<ul>\\n<li>“<a title=\"Permanent link to Of precious swords and Old Sinitic reconstructions\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=24466\" target=\"_blank\" rel=\"bookmark noopener\">Of precious swords and Old Sinitic reconstructions</a>” (3/8/16)</li>\\n<li>“<a title=\"Permanent link to Of precious swords and Old Sinitic reconstructions, part 2\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=24595\" target=\"_blank\" rel=\"bookmark noopener\">Of precious swords and Old Sinitic reconstructions, part 2</a>” (3/12/16)</li>\\n<li>“<a title=\"Permanent link to Of precious swords and Old Sinitic reconstructions, part 3\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=24705\" target=\"_blank\" rel=\"bookmark noopener\">Of precious swords and Old Sinitic reconstructions, part 3</a>” (3/16/16)</li>\\n<li>“<a title=\"Permanent link to Of precious swords and Old Sinitic reconstructions, part 4\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=24831\" rel=\"bookmark\">Of precious swords and Old Sinitic reconstructions, part 4</a>” (3/24/16)</li>\\n<li>\"<a title=\"Permanent link to Of precious swords and Old Sinitic reconstructions, part 5\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=24918\" rel=\"bookmark\">Of precious swords and Old Sinitic reconstructions, part 5</a>\" (3/28/16)</li>\\n<li>\"<a title=\"Permanent link to Of armaments and Old Sinitic reconstructions, part 6\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=35845\" rel=\"bookmark\">Of armaments and Old Sinitic reconstructions, part 6</a>\" (12/23/17)</li>\\n<li>\"<a title=\"Permanent link to Of shumai and Old Sinitic reconstructions\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=26756\" rel=\"bookmark\">Of shumai and Old Sinitic reconstructions</a>\" (7/19/16)</li>\\n<li>\"<a title=\"Permanent link to Of felt hats, feathers, macaroni, and weasels\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=24590\" rel=\"bookmark\">Of felt hats, feathers, macaroni, and weasels</a>\" (3/13/16)</li>\\n<li>\"<a title=\"Permanent link to GA\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=33963\" rel=\"bookmark\">GA</a>\" (8/6/17)</li>\\n<li>\"<a title=\"Permanent link to Of dogs and Old Sinitic reconstructions\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=36996\" rel=\"bookmark\">Of dogs and Old Sinitic reconstructions</a>\" (3/17/18)</li>\\n<li>\"<a title=\"Permanent link to Of ganders, geese, and Old Sinitic reconstructions\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=40459\" rel=\"bookmark\">Of ganders, geese, and Old Sinitic reconstructions</a>\" (10/29/18)</li>\\n<li>\"<a title=\"Permanent link to Eurasian eureka\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=28020\" rel=\"bookmark\">Eurasian eureka</a>\" (9/12/16)</li>\\n<li>\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=40683\">Of knots, pimples, and Sinitic reconstructions</a>\" (11/12/18)</li>\\n<li>\"<a title=\"Permanent link to Of jackal and hide and Old Sinitic reconstructions\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=41045\" rel=\"bookmark\">Of jackal and hide and Old Sinitic reconstructions</a>\" (12/16/18)</li>\\n<li>\"<a title=\"Permanent link to Of reindeer and Old Sinitic reconstructions\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=41164\" rel=\"bookmark\">Of reindeer and Old Sinitic reconstructions</a>\" (12/23/18)</li>\\n<li>\"<a title=\"Permanent link to Galactic glimmers: of milk and Old Sinitic reconstructions\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=41346\" rel=\"bookmark\">Galactic glimmers: of milk and Old Sinitic reconstructions</a>\" (1/8/19)</li>\\n<li>\"<a title=\"Permanent link to An early fourth century AD historical puzzle involving a Caucasian people in North China\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=41519\" rel=\"bookmark\">An early fourth century AD historical puzzle involving a Caucasian people in North China</a>\" (12/25/19)</li>\\n<li>\"<a title=\"Permanent link to Thai \" href=\"http://languagelog.ldc.upenn.edu/nll/?p=41538\" rel=\"bookmark\">Thai \\'khwan\\' (\\'soul\\') and Old Sinitic reconstructions</a>\" (1/28/19)</li>\\n</ul>'}],\n",
       "   'thr_total': '29'},\n",
       "  {'authors': [{'name': 'Victor Mair',\n",
       "     'href': 'https://www.sas.upenn.edu/ealc/mair'}],\n",
       "   'author_detail': {'name': 'Victor Mair',\n",
       "    'href': 'https://www.sas.upenn.edu/ealc/mair'},\n",
       "   'href': 'https://www.sas.upenn.edu/ealc/mair',\n",
       "   'author': 'Victor Mair',\n",
       "   'title': '&quot;Hello&quot; sung by a Kazakh',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': '&quot;Hello&quot; sung by a Kazakh'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41787'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41787#comments',\n",
       "     'count': '10',\n",
       "     'thr:count': '10'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41787',\n",
       "     'count': '10',\n",
       "     'thr:count': '10'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41787',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41787',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-15T01:24:14Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=15, tm_hour=1, tm_min=24, tm_sec=14, tm_wday=4, tm_yday=46, tm_isdst=0),\n",
       "   'published': '2019-02-15T01:24:14Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=15, tm_hour=1, tm_min=24, tm_sec=14, tm_wday=4, tm_yday=46, tm_isdst=0),\n",
       "   'tags': [{'term': 'Language and music',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Multilingualism',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Second language',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'Here is Dimash Kudaibergen singing \"Hello\": His voice is electrifying, rich with emotion and having an incredible range of 6 octaves and 2 semitones from C2 to D8. As with Enkh Erdene, the Mongolian cowboy, singing \"Amarillo by Morning\", this is an astonishing performance.\\xa0 Dimash has performed songs in twelve different languages:\\xa0 his native language [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Here is Dimash Kudaibergen singing \"Hello\": His voice is electrifying, rich with emotion and having an incredible range of 6 octaves and 2 semitones from C2 to D8. As with Enkh Erdene, the Mongolian cowboy, singing \"Amarillo by Morning\", this is an astonishing performance.\\xa0 Dimash has performed songs in twelve different languages:\\xa0 his native language [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41787',\n",
       "     'value': '<p>Here is Dimash Kudaibergen singing \"<a href=\"https://en.wikipedia.org/wiki/Hello_%28Adele_song%29\">Hello</a>\":</p>\\n<p><iframe width=\"500\" height=\"281\" src=\"https://www.youtube.com/embed/9G92IVn8DIw?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe></p>\\n<p><span id=\"more-41787\"></span></p>\\n<p>His voice is electrifying, rich with emotion and having an incredible range of 6 octaves and 2 semitones from C2 to D8.</p>\\n<p>As with <a href=\"http://languagelog.ldc.upenn.edu/nll/?p=41689\">Enkh Erdene, the Mongolian cowboy, singing \"Amarillo by Morning\"</a>, this is an astonishing performance.\\xa0 Dimash has performed songs in twelve different languages:\\xa0 his native language Kazakh, as well as Russian, Mandarin, English, French, Ukrainian, Italian, Kyrgyz, Turkish, Serbian, German, and Spanish (<a href=\"https://en.wikipedia.org/wiki/Dimash_Kudaibergen\">source</a>).\\xa0 He speaks Kazakh and Russian, and studies English and Mandarin.\\xa0 Although his rendition of \"Hello\" sounds natural and is easy to understand, I can detect many points at which his English has non-native nuances.\\xa0 Nonetheless, it is remarkably good for being only one of the dozen different languages in which he sings.</p>\\n<p>I hope that Language Log readers will track down his performances in other languages.\\xa0 I\\'m especially curious to hear what he sounds like when singing in Mandarin and Russian (which is probably virtually native).</p>\\n<p>[h.t. Judy Chang]</p>'}],\n",
       "   'thr_total': '10'},\n",
       "  {'authors': [{'name': 'Ben Zimmer', 'href': 'http://benzimmer.com/'}],\n",
       "   'author_detail': {'name': 'Ben Zimmer', 'href': 'http://benzimmer.com/'},\n",
       "   'href': 'http://benzimmer.com/',\n",
       "   'author': 'Ben Zimmer',\n",
       "   'title': 'Calling (a) moose',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Calling (a) moose'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41814'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41814#comments',\n",
       "     'count': '29',\n",
       "     'thr:count': '29'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41814',\n",
       "     'count': '29',\n",
       "     'thr:count': '29'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41814',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41814',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-14T19:09:49Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=14, tm_hour=19, tm_min=9, tm_sec=49, tm_wday=3, tm_yday=45, tm_isdst=0),\n",
       "   'published': '2019-02-14T16:27:38Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=14, tm_hour=16, tm_min=27, tm_sec=38, tm_wday=3, tm_yday=45, tm_isdst=0),\n",
       "   'tags': [{'term': 'ambiguity',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Animal communication',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Syntax',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'Headline from the Bangor Daily News (Feb. 13, 2019): \"Maine now holds the world record for most people calling a moose at the same time.\" Screenshot for posterity: Update: The headline has been changed to read, \"Maine now holds the world record for simultaneous moose-calling.\" The body of the article reveals that the moose-calling record [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Headline from the Bangor Daily News (Feb. 13, 2019): \"Maine now holds the world record for most people calling a moose at the same time.\" Screenshot for posterity: Update: The headline has been changed to read, \"Maine now holds the world record for simultaneous moose-calling.\" The body of the article reveals that the moose-calling record [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41814',\n",
       "     'value': '<p>Headline from the <em><a href=\"https://bangordailynews.com/2019/02/13/outdoors/maine-now-holds-the-world-record-for-simultaneous-moose-calling/\">Bangor Daily News</a></em> (Feb. 13, 2019): \"<strong>Maine now holds the world record for most people calling a moose at the same time</strong>.\"</p>\\n<p>Screenshot for posterity:</p>\\n<p align=\"center\"><a href=\"https://bangordailynews.com/2019/02/13/outdoors/maine-now-holds-the-world-record-for-simultaneous-moose-calling\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/~bgzimmer/callingmoose.jpg\" alt=\"\" width=\"475\" /></a></p>\\n<p><em>Update</em>: The headline has been changed to read, \"<a href=\"https://bangordailynews.com/2019/02/13/outdoors/maine-now-holds-the-world-record-for-simultaneous-moose-calling/\">Maine now holds the world record for simultaneous moose-calling.</a>\"<br />\\n<span id=\"more-41814\"></span></p>\\n<p>The body of the article reveals that the moose-calling record did not involve people calling one particular moose:</p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #000080;\">Last June, when thousands of people flocked to the Skowhegan Fairgrounds for the Maine Moose Festival, many took part in an offbeat attempt to set a world record.</span></p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #000080;\">More on that in a minute. First, a question: Did you know there was a world record for the most people calling moose at the same time? The answer: Until now, there wasn’t.</span></p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #000080;\">Now, there is.</span></p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #000080;\">On Wednesday, organizers of that festival announced that during that record-setting attempt, a total of 1,054 people grunted, wailed and otherwise channeled their inner moose, and that effort has been confirmed by Guinness World Records, according to a press release.</span></p>\\n<p>The article refers to the \"world record for the most people calling moose at the same time,\" but for the headline, \"calling moose\" got changed to \"calling a moose.\" What happened? I think the difficulty lies in the fact that the plural of\\xa0<em>moose </em>is\\xa0<em>moose</em> &#8212; in other words,\\xa0<em>moose</em> is a zero plural. \"Calling moose\" would then be appropriate for the act of calling (plural) moose in general.</p>\\n<p>But because it\\'s not overtly marked as plural,\\xa0<em>moose</em> in this context is easily misconstrued as singular. \"Calling (singular) moose\" isn\\'t quite grammatical, instead bringing to mind Boris and Natasha\\'s endless vendetta against \"moose and squirrel\" on\\xa0<em>Rocky and Bullwinkle</em>. In a modern take, <em>New Yorker </em>cartoonist <a href=\"https://www.newyorker.com/cartoons/daily-cartoon/wednesday-march-29th-natasha-boris-fake-news\">Pat Byrnes</a>\\xa0imagines Boris and Natasha hovering over a tweeting Donald Trump, with Boris whispering \"Tell them is fake news, work of moose and squirrel.\"</p>\\n<p align=\"center\"><a href=\"https://www.newyorker.com/cartoons/daily-cartoon/wednesday-march-29th-natasha-boris-fake-news\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/~bgzimmer/mooseandsquirrel.jpg\" alt=\"\" width=\"475\" /></a></p>\\n<p>In the Russian-inflected English of Boris Badenov, articles get lost, leading to the <a href=\"http://languagelog.ldc.upenn.edu/nll/?p=2210\">anarthrous</a> construction, \"moose and squirrel.\" I\\'m reminded of a topic that I wrote about for <a href=\"https://www.theatlantic.com/ideas/archive/2018/12/we-need-wall-why-kirstjen-nielsen-sounds-like-hulk/578845/\"><em>The Atlantic</em></a> in December, on how Trump\\'s plans for a border wall often get discussed by the president and his administration using\\xa0<em>wall</em> as a mass noun. So when Homeland Security Secretary Kirstjen Nielsen told Congress \"<a href=\"https://twitter.com/joshtpm/status/1075839997687803905\">We need wall</a>,\" it led many on Twitter to compare her to stereotypically anarthrous speakers like the Incredible Hulk. And of course there were lots of jokes about how Nielsen\\'s plea was taken straight from Russian, since (as we know from Boris and Natasha) <a href=\"https://www.researchgate.net/publication/271389975_Article_use_in_L2_English_by_L1_Russian_and_L1_German_speakers\">native Russian speakers</a> often leave out articles in L2 English.</p>\\n<p>So my guess is an editor thought \"calling moose\" ran the risk of sounding too much like Boris (or the Hulk), and the article \"a\" was inserted in the headline &#8212; making it sound like there was a specific moose being called. Of course, all of this could have been avoided by referring to \"moose-calling\" instead, since that doesn\\'t rely on a plural form. Indeed, as noted above, the <a href=\"http://maine-now-holds-the-world-record-for-simultaneous-moose-calling\"><em>Bangor Daily News\\xa0</em> headline</a> was eventually changed to read, \"Maine now holds the world record for simultaneous moose-calling,\" which is a better way to phrase it. And the <a href=\"https://mainstreetskowhegan.org/guinness-world-record/\">announcement of the record</a> from the Skowhegan Moose Festival read, \"We did it! Guinness World Records has confirmed that Main Street Skowhegan and the Town of Skowhegan set the new world record for the most people moose calling simultaneously!\"</p>\\n<p>Here\\'s what the record-setting moose call sounded like, in case you were wondering. No singular or plural moose attended.</p>\\n<p><iframe style=\"border: none; overflow: hidden;\" src=\"https://www.facebook.com/plugins/video.php?href=https%3A%2F%2Fwww.facebook.com%2FSkowheganMooseFest%2Fvideos%2F257835511623084%2F&amp;show_text=0&amp;width=475\" width=\"475\" height=\"267\" frameborder=\"0\" scrolling=\"no\" allowfullscreen=\"allowfullscreen\"></iframe></p>\\n<p>(Hat tip to <a href=\"https://www.facebook.com/JackLynch00/posts/10156066316292393\">Jack Lynch</a>, who assiduously monitors all moose-related news.)</p>\\n<p><em>Related readings:</em></p>\\n<ul>\\n<li>\"<a href=\"http://itre.cis.upenn.edu/~myl/languagelog/archives/003879.html\">Plural, Mass, Collective\"</a> (AZ, Nov. 8, 2006)</li>\\n<li>\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=203\">Chad back in the news</a>\" (BZ, May 30, 2008)</li>\\n<li>\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=777\">Zero relationships</a>\" (AZ, Oct. 27, 2008)</li>\\n<li>\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=2210\">Anarthrous irony</a>\" (ML, Mar. 27, 2010)</li>\\n<li>\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=4017\">Peeve emergence: The case of \\'vinyls\\'</a>\" (ML, June 12, 2012)</li>\\n<li>\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=4305\">Too much Obama vote</a>\" (BZ, Nov. 7, 2012)</li>\\n<li>Arnold Zwicky\\'s <a href=\"https://arnoldzwicky.org/2011/03/14/arthrousness/\">inventory of blog posts about arthrousness</a></li>\\n</ul>'}],\n",
       "   'thr_total': '29'},\n",
       "  {'authors': [{'name': 'Mark Liberman',\n",
       "     'href': 'http://ling.upenn.edu/~myl'}],\n",
       "   'author_detail': {'name': 'Mark Liberman',\n",
       "    'href': 'http://ling.upenn.edu/~myl'},\n",
       "   'href': 'http://ling.upenn.edu/~myl',\n",
       "   'author': 'Mark Liberman',\n",
       "   'title': 'Contextualized Muppet Embeddings',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Contextualized Muppet Embeddings'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41805'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41805#comments',\n",
       "     'count': '11',\n",
       "     'thr:count': '11'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41805',\n",
       "     'count': '11',\n",
       "     'thr:count': '11'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41805',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41805',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-13T22:34:40Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=13, tm_hour=22, tm_min=34, tm_sec=40, tm_wday=2, tm_yday=44, tm_isdst=0),\n",
       "   'published': '2019-02-13T11:39:04Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=13, tm_hour=11, tm_min=39, tm_sec=4, tm_wday=2, tm_yday=44, tm_isdst=0),\n",
       "   'tags': [{'term': 'Changing times',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'Over the past few years, it\\'s been increasingly common for computational linguists to use various kinds of \"word embeddings\". The foundation for this was the vector space model, developed in the 1960s for document retrieval applications, which represents a piece of text as a vector of word (or \"term\") counts. The next step was latent [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Over the past few years, it\\'s been increasingly common for computational linguists to use various kinds of \"word embeddings\". The foundation for this was the vector space model, developed in the 1960s for document retrieval applications, which represents a piece of text as a vector of word (or \"term\") counts. The next step was latent [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41805',\n",
       "     'value': '<p>Over the past few years, it\\'s been increasingly common for computational linguists to use various kinds of \"<a href=\"https://en.wikipedia.org/wiki/Word_embedding\" target=\"_blank\" rel=\"noopener\">word embeddings</a>\".</p>\\n<p>The foundation for this was the <em><a href=\"https://en.wikipedia.org/wiki/Vector_space_model\" target=\"_blank\" rel=\"noopener\">vector space model</a></em>, developed in the 1960s for document retrieval applications, which represents a piece of text as a vector of word (or \"term\") counts. The next step was <a href=\"https://en.wikipedia.org/wiki/Latent_semantic_analysis\" target=\"_blank\" rel=\"noopener\"><em>latent semantic analysi</em>s</a>, developed in the 1980s, which orthogonalizes the term-by-document matrix (via singular value decomposition) and retains only a few hundred of the most important dimensions. Among other benefits, this provides a sort of \"soft thesaurus\", since words that tend to co-occur will be relatively close in the resulting space. Then in the 2000s came a wide variety of other ways of turning large text collections into vector-space dictionaries, representing each word as vector of numbers derived in some way from the contexts in which it occurs &#8212; some widely used examples from the 2010s include <a href=\"https://arxiv.org/abs/1310.4546\" target=\"_blank\" rel=\"noopener\">word2vec</a> and <a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\" rel=\"noopener\">GloVe</a>\\xa0(\"Global Vectors for Word Representation\").</p>\\n<p><span id=\"more-41805\"></span></p>\\n<p>The latest trend is for \"contextualized\" word representations, in which each word is represented by an array of numbers that depends not only on its distribution in training texts, but also on its context in the particular case being analyzed. Three examples emerged in 2018:</p>\\n<ul>\\n<li>ULMFiT (\"<a href=\"https://arxiv.org/abs/1801.06146\" target=\"_blank\" rel=\"noopener\">Universal Language Model Fine-tuning</a>\")</li>\\n<li>ELMo (\"<a href=\"https://arxiv.org/pdf/1802.05365.pdf\" target=\"_blank\" rel=\"noopener\">Embeddings from Language Models</a>\")</li>\\n<li>BERT (\"<a href=\"https://arxiv.org/abs/1810.04805\" target=\"_blank\" rel=\"noopener\">Bidirectional Encoder Representations from Transformers</a>\")</li>\\n</ul>\\n<p>The emerging pattern is obvious. There will be more contextualized word-embedding methods &#8212; and soon we can expect to see the acronyms ERNiE, GRoVEr, KERMiT, &#8230;</p>\\n<p>Update &#8212;\\xa0 Yuval Pinter was way ahead of me&#8230;</p>\\n<blockquote class=\"twitter-tweet\" data-width=\"500\" data-dnt=\"true\">\\n<p lang=\"en\" dir=\"ltr\">Several New Ultimate Feature Finders Letting Embeddings Use Procedurally Acquired Global Universal Structure <a href=\"https://t.co/NRc3qPFAKP\">https://t.co/NRc3qPFAKP</a></p>\\n<p>&mdash; Yuval Pinter (@yuvalpi) <a href=\"https://twitter.com/yuvalpi/status/1050751455437697024?ref_src=twsrc%5Etfw\">October 12, 2018</a></p></blockquote>\\n<p><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></p>\\n<p>&nbsp;</p>'}],\n",
       "   'thr_total': '11'},\n",
       "  {'authors': [{'name': 'Mark Liberman',\n",
       "     'href': 'http://ling.upenn.edu/~myl'}],\n",
       "   'author_detail': {'name': 'Mark Liberman',\n",
       "    'href': 'http://ling.upenn.edu/~myl'},\n",
       "   'href': 'http://ling.upenn.edu/~myl',\n",
       "   'author': 'Mark Liberman',\n",
       "   'title': 'Too few words to describe emotions',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Too few words to describe emotions'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41797'},\n",
       "    {'href': 'http://languagelog.ldc.upenn.edu/myl/FewWordsForAnxiety0.wav',\n",
       "     'rel': 'enclosure',\n",
       "     'length': '751660',\n",
       "     'type': 'audio/wav'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41797#comments',\n",
       "     'count': '38',\n",
       "     'thr:count': '38'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41797',\n",
       "     'count': '38',\n",
       "     'thr:count': '38'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41797',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41797',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-13T11:41:01Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=13, tm_hour=11, tm_min=41, tm_sec=1, tm_wday=2, tm_yday=44, tm_isdst=0),\n",
       "   'published': '2019-02-12T23:54:14Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=12, tm_hour=23, tm_min=54, tm_sec=14, tm_wday=1, tm_yday=43, tm_isdst=0),\n",
       "   'tags': [{'term': 'Psychology of language',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Words words words',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'At about 22:45 of the BBC discussion program The Moral Maze,\\xa0Natasha Devon\\xa0\\xa0asserts Your browser does not support the audio element. Well it- I- again, one of the problems is language, actually, because in English, we have a very limited emotional vocabulary. When you look at other languages, they- they have a much broader amount of [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'At about 22:45 of the BBC discussion program The Moral Maze,\\xa0Natasha Devon\\xa0\\xa0asserts Your browser does not support the audio element. Well it- I- again, one of the problems is language, actually, because in English, we have a very limited emotional vocabulary. When you look at other languages, they- they have a much broader amount of [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41797',\n",
       "     'value': '<p>At about 22:45 of the <a href=\"https://www.bbc.co.uk/programmes/m0002c37\" target=\"_blank\" rel=\"noopener\">BBC discussion program The Moral Maze</a>,\\xa0<a href=\"https://en.wikipedia.org/wiki/Natasha_Devon\" target=\"_blank\" rel=\"noopener\">Natasha Devon\\xa0</a>\\xa0asserts</p>\\n<div style=\"padding-left: 30px;\"><audio style=\"width: 230px;\" controls=\"controls\"><source src=\"http://languagelog.ldc.upenn.edu/myl/FewWordsForAnxiety0.wav\" type=\"audio/wav\" />Your browser does not support the audio element.</audio></div>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #800000;\">Well it- I- again, one of the problems is language, actually, because in English, we have a very limited emotional vocabulary. When you look at other languages, they- they have a much broader amount of words that they can use to describe their emotions and their mental health. So, if I say to you ‘I’m feeling anxious’, that could be anything from common or garden anxiety right through to an anxiety disorder. And one is a medical issue and the other is not.</span></p>\\n<p><span id=\"more-41797\"></span></p>\\n<p>E.Z. Granet, who sent the link, comments</p>\\n<p style=\"padding-left: 30px;\"><span style=\"color: #000080;\">My initial reaction was one of mild offence, as this program was focused on rising suicide and self-harm rates in Britain, and Ms Devon seemed to be blaming this quite serious social problem on the fact that English inhibits self-expression. My second reaction was to be linguistically intrigued, as I don’t think I’ve ever heard someone express the common “no word for X” fallacy about their own native language! One would think that someone who worked in mental health would have plenty of experience with precisely how expressive English can be…</span></p>\\n<p>In response to \"anxious\", thesaurus.com offers <a href=\"https://www.thesaurus.com/browse/anxious\" target=\"_blank\" rel=\"noopener\">42 alternative choices</a>\\xa0of words or short phrases:</p>\\n<p><a href=\"http://languagelog.ldc.upenn.edu/myl/AnxiousThesaurus.png\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/myl/AnxiousThesaurus.png\" width=\"490\" /></a></p>\\n<p>The DSM chapter on \"<a href=\"https://dsm.psychiatryonline.org/doi/10.1176/appi.books.9780890425596.dsm05\" target=\"_blank\" rel=\"noopener\">Anxiety Disorders</a>\" gives a long and detailed discussion of symptoms and circumstances, comprising more than 29,000 words.</p>\\n<p>Despite these linguistic and analytic resources,\\xa0it\\'s clear that most people find it hard to describe their mental state in a clear way, even to themselves, and that it\\'s usually harder for them to understand other people\\'s self descriptions. But is this the fault of the English language?\\xa0And is it really true that \"other languages &#8230; have a much broader amount of words that they can use to describe their emotions and their mental health\"?</p>\\n<p>[Added to the \"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=1081\" target=\"_blank\" rel=\"noopener\">\\'No word for X\\' archive</a>\"]</p>'}],\n",
       "   'thr_total': '38'},\n",
       "  {'authors': [{'name': 'Victor Mair',\n",
       "     'href': 'https://www.sas.upenn.edu/ealc/mair'}],\n",
       "   'author_detail': {'name': 'Victor Mair',\n",
       "    'href': 'https://www.sas.upenn.edu/ealc/mair'},\n",
       "   'href': 'https://www.sas.upenn.edu/ealc/mair',\n",
       "   'author': 'Victor Mair',\n",
       "   'title': 'Only the Communist Party can save the earth',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Only the Communist Party can save the earth'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41768'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41768#comments',\n",
       "     'count': '13',\n",
       "     'thr:count': '13'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41768',\n",
       "     'count': '13',\n",
       "     'thr:count': '13'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41768',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41768',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-12T20:51:04Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=12, tm_hour=20, tm_min=51, tm_sec=4, tm_wday=1, tm_yday=43, tm_isdst=0),\n",
       "   'published': '2019-02-12T20:51:04Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=12, tm_hour=20, tm_min=51, tm_sec=4, tm_wday=1, tm_yday=43, tm_isdst=0),\n",
       "   'tags': [{'term': 'Language and the movies',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'Movie ticket for \"Liúlàng dìqiú 流浪地球\" (\"Wandering earth\"): (Source) The line at the bottom of the ticket reads: zhǐyǒu gòngchǎndǎng kěyǐ jiù dìqiú 只有共产党可以救地球 \"Only the Communist Party can save the earth\" From a Chinese graduate student: The line at bottom shows the degree to which the theaters are desperate to survive under the tension [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Movie ticket for \"Liúlàng dìqiú 流浪地球\" (\"Wandering earth\"): (Source) The line at the bottom of the ticket reads: zhǐyǒu gòngchǎndǎng kěyǐ jiù dìqiú 只有共产党可以救地球 \"Only the Communist Party can save the earth\" From a Chinese graduate student: The line at bottom shows the degree to which the theaters are desperate to survive under the tension [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41768',\n",
       "     'value': '<p>Movie ticket for \"Liúlàng dìqiú 流浪地球\" (\"Wandering earth\"):</p>\\n<p align=\"center\"><a href=\"http://languagelog.ldc.upenn.edu/~bgzimmer/savetheearth.jpg\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/~bgzimmer/savetheearth.jpg\" alt=\"\" width=\"350\" /></a><br />\\n(<a href=\"https://pbs.twimg.com/media/Dyx0245V4AAW1SN.jpg\" target=\"_blank\" rel=\"noopener\" data-saferedirecturl=\"https://www.google.com/url?q=https://pbs.twimg.com/media/Dyx0245V4AAW1SN.jpg&amp;source=gmail&amp;ust=1550028781517000&amp;usg=AFQjCNE6TDovp1UGnSKTLZlY5DKOsTqR3Q\">Source</a>)<span id=\"more-41768\"></span></p>\\n<p>The line at the bottom of the ticket reads:</p>\\n<p style=\"padding-left: 30px;\">zhǐyǒu gòngchǎndǎng kěyǐ jiù dìqiú</p>\\n<p style=\"padding-left: 30px;\">只有共产党可以救地球</p>\\n<p style=\"padding-left: 30px;\">\"<span class=\"m_4476359369037513025tlid-translation m_4476359369037513025translation\"><span title=\"\">Only the Communist Party can save the earth</span></span><span class=\"m_4476359369037513025tlid-translation-gender-indicator m_4476359369037513025translation-gender-indicator\">\"</span></p>\\n<p>From a Chinese graduate student:</p>\\n<div>\\n<p style=\"padding-left: 30px;\">The line at bottom shows the degree to which the theaters are desperate to survive under the tension between imaginative works and political sensitivity, almost a perfect analogy to the “smoking is harmful to your health” notices in every cigarette commercial.</p>\\n<p style=\"padding-left: 30px;\">But \"Liúlàng dìqiú 流浪地球\" (\"<span class=\"m_4476359369037513025tlid-translation m_4476359369037513025translation\"><span title=\"\">Wandering earth\")</span></span>\\xa0 is said to be a great sci-fi movie. I’m very interested in\\xa0<a href=\"https://en.wikipedia.org/wiki/Liu_Cixin\" target=\"_blank\" rel=\"noopener\" data-saferedirecturl=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Liu_Cixin&amp;source=gmail&amp;ust=1550028781517000&amp;usg=AFQjCNEVbeLZERG0m1YlybhAT1HAy77tRQ\">Liu Cixin</a>’s works. He is a Hugo Award winner!\\xa0 I\\'m looking forward to seeing the movie this weekend.</p>\\n<p><b>Readings</b></p>\\n<p>\"<a title=\"Permanent link to Ken Liu reinvents Chinese characters\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=29652\" target=\"_blank\" rel=\"bookmark noopener\" data-saferedirecturl=\"https://www.google.com/url?q=http://languagelog.ldc.upenn.edu/nll/?p%3D29652&amp;source=gmail&amp;ust=1550028781517000&amp;usg=AFQjCNF1ZP0udN9-gmIJzS47tn01yDSNsQ\">Ken Liu reinvents Chinese characters</a>\" (12/5/16)</p>\\n<p>\"<a title=\"Permanent link to Ball ball 你\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=34788\" target=\"_blank\" rel=\"bookmark noopener\" data-saferedirecturl=\"https://www.google.com/url?q=http://languagelog.ldc.upenn.edu/nll/?p%3D34788&amp;source=gmail&amp;ust=1550028781517000&amp;usg=AFQjCNFj5JBIBSbOWsyU8fbrQtpH-48ZUg\">Ball ball 你</a>\" (10/4/17) &#8212; third comment from bottom</p>\\n<p class=\"m_4476359369037513025AOC-Subtitle\">\"<a href=\"https://www.scmp.com/news/china/society/article/2185703/chinese-sci-fi-blockbuster-wandering-earth-battles-horde\" target=\"_blank\" rel=\"noopener\" data-saferedirecturl=\"https://www.google.com/url?q=https://www.scmp.com/news/china/society/article/2185703/chinese-sci-fi-blockbuster-wandering-earth-battles-horde&amp;source=gmail&amp;ust=1550028781518000&amp;usg=AFQjCNEtUxjE7z6sEN6bvEhZU6NCOobC4w\">Chinese sci-fi blockbuster The Wandering Earth battles horde of rampaging online pirates</a>\"<span class=\"m_4476359369037513025titletext\">, South China Morning Post (2/11/19)</span><u></u><u></u>:\\xa0 China’s latest blockbuster\\xa0<i>The Wandering Earth</i>\\xa0is making megabucks at the box office but is fighting a bigger threat than a looming explosion of the sun – a voracious piracy industry that is eating into its ticket takings.</p>\\n<p>[h.t. Geoff Wade; thanks to Qing Liao]</p>\\n<pre class=\"m_4476359369037513025moz-signature\"></pre>\\n<pre class=\"m_4476359369037513025moz-signature\"></pre>\\n</div>'}],\n",
       "   'thr_total': '13'},\n",
       "  {'authors': [{'name': 'Mark Liberman',\n",
       "     'href': 'http://ling.upenn.edu/~myl'}],\n",
       "   'author_detail': {'name': 'Mark Liberman',\n",
       "    'href': 'http://ling.upenn.edu/~myl'},\n",
       "   'href': 'http://ling.upenn.edu/~myl',\n",
       "   'author': 'Mark Liberman',\n",
       "   'title': 'Portentous periods',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Portentous periods'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41791'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41791#comments',\n",
       "     'count': '25',\n",
       "     'thr:count': '25'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41791',\n",
       "     'count': '25',\n",
       "     'thr:count': '25'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41791',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41791',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-12T11:33:32Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=12, tm_hour=11, tm_min=33, tm_sec=32, tm_wday=1, tm_yday=43, tm_isdst=0),\n",
       "   'published': '2019-02-12T11:33:32Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=12, tm_hour=11, tm_min=33, tm_sec=32, tm_wday=1, tm_yday=43, tm_isdst=0),\n",
       "   'tags': [{'term': 'Punctuation',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': \"Further developments in the indexicality (intexticality?) of punctuation: Friend sends me a chat message that&#39;s just &#34;Hey.&#34; Me: oh my god what&#39;s with the period has someone died (Reader, it was fine.) &#8212; Gretchen McCulloch (@GretchenAMcC) February 7, 2019 This augurs well for the success of Gretchen McCullough's forthcoming book, Because Internet. Some background on [&#8230;]\",\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': \"Further developments in the indexicality (intexticality?) of punctuation: Friend sends me a chat message that&#39;s just &#34;Hey.&#34; Me: oh my god what&#39;s with the period has someone died (Reader, it was fine.) &#8212; Gretchen McCulloch (@GretchenAMcC) February 7, 2019 This augurs well for the success of Gretchen McCullough's forthcoming book, Because Internet. Some background on [&#8230;]\"},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41791',\n",
       "     'value': '<p>Further developments in the indexicality (intexticality?) of punctuation:</p>\\n<blockquote class=\"twitter-tweet\" data-width=\"500\" data-dnt=\"true\">\\n<p lang=\"en\" dir=\"ltr\">Friend sends me a chat message that&#39;s just &quot;Hey.&quot; </p>\\n<p>Me: oh my god what&#39;s with the period has someone died </p>\\n<p>(Reader, it was fine.)</p>\\n<p>&mdash; Gretchen McCulloch (@GretchenAMcC) <a href=\"https://twitter.com/GretchenAMcC/status/1093584120846970880?ref_src=twsrc%5Etfw\">February 7, 2019</a></p></blockquote>\\n<p><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script></p>\\n<p><span id=\"more-41791\"></span></p>\\n<p>This augurs well for the success of Gretchen McCullough\\'s forthcoming book, <a href=\"https://gretchenmcculloch.com/book/\" target=\"_blank\" rel=\"noopener\">Because Internet</a>.</p>\\n<p>Some background on the topic of orthographic semiotics:</p>\\n<p style=\"padding-left: 30px;\">\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=4304\" target=\"_blank\" rel=\"noopener\">The new semiotics of punctuation</a>\", 11/7/2012<br />\\n\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=8667\" target=\"_blank\" rel=\"noopener\">Aggressive periods and the popularity of linguistics</a>\", 11/26/2013<br />\\n\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=13723\" target=\"_blank\" rel=\"noopener\">Generational punctuation differences again</a>\", 8/1/2014<br />\\n\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=17869\" target=\"_blank\" rel=\"noopener\">Query: Punctuation in personal digital media</a>\", 2/23/2015<br />\\n\"<a href=\"http://languagelog.ldc.upenn.edu/nll/?p=21548\" target=\"_blank\" rel=\"noopener\">Anticipatory confirmation</a>\", 10/7/2015</p>\\n<p>&nbsp;</p>'}],\n",
       "   'thr_total': '25'},\n",
       "  {'authors': [{'name': 'Victor Mair',\n",
       "     'href': 'https://www.sas.upenn.edu/ealc/mair'}],\n",
       "   'author_detail': {'name': 'Victor Mair',\n",
       "    'href': 'https://www.sas.upenn.edu/ealc/mair'},\n",
       "   'href': 'https://www.sas.upenn.edu/ealc/mair',\n",
       "   'author': 'Victor Mair',\n",
       "   'title': 'The consequence is proud',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'The consequence is proud'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41762'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41762#comments',\n",
       "     'count': '5',\n",
       "     'thr:count': '5'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41762',\n",
       "     'count': '5',\n",
       "     'thr:count': '5'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41762',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41762',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-11T17:14:21Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=11, tm_hour=17, tm_min=14, tm_sec=21, tm_wday=0, tm_yday=42, tm_isdst=0),\n",
       "   'published': '2019-02-11T17:13:53Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=11, tm_hour=17, tm_min=13, tm_sec=53, tm_wday=0, tm_yday=42, tm_isdst=0),\n",
       "   'tags': [{'term': 'Lost in translation',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Signs',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'From Yixue Yang: No problem with the top half of the sign, but the bottom is a disaster.\\xa0 The Chinese says: hòuguǒ zìfù 后果自负 (\"[you your]self will be responsible for the consequences\") The mistranslation of zìfù 自负 as \"proud\" comes from the fact that it means both \"bear the responsibility oneself\" and \"be conceited\". Microsoft [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'From Yixue Yang: No problem with the top half of the sign, but the bottom is a disaster.\\xa0 The Chinese says: hòuguǒ zìfù 后果自负 (\"[you your]self will be responsible for the consequences\") The mistranslation of zìfù 自负 as \"proud\" comes from the fact that it means both \"bear the responsibility oneself\" and \"be conceited\". Microsoft [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41762',\n",
       "     'value': '<p>From Yixue Yang:</p>\\n<p align=\"center\"><a href=\"http://languagelog.ldc.upenn.edu/~bgzimmer/consequence.jpg\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/~bgzimmer/consequence.jpg\" alt=\"\" width=\"400\" /></a><span id=\"more-41762\"></span></p>\\n<p>No problem with the top half of the sign, but the bottom is a disaster.\\xa0 The Chinese says:</p>\\n<p style=\"padding-left: 30px;\">hòuguǒ zìfù 后果自负<br />\\n(\"[you your]self will be responsible for the consequences\")</p>\\n<p>The mistranslation of zìfù 自负 as \"proud\" comes from the fact that it means both \"bear the responsibility oneself\" and \"be conceited\".</p>\\n<p>Microsoft Translator gives \"The consequences are conceited\".</p>\\n<p>Baidu Fanyi has \"Consequence conceit\".</p>\\n<p>Google Translate has \"\\xa0<span class=\"m_-326944538476483258tlid-translation m_-326944538476483258translation\"><span title=\"\">Conceited at your own risk\".</span></span></p>\\n<p>In other words, they all produce gibberish.\\xa0 Once again, I must stress that machine translation does not work for Literary Sinitic / Classical Chinese, though it certainly engenders a lot of mirth.</p>\\n<p><b>Reading</b></p>\\n<p>\"<a title=\"Permanent link to \" href=\"http://languagelog.ldc.upenn.edu/nll/?p=41594\" target=\"_blank\" rel=\"bookmark noopener\" data-saferedirecturl=\"https://www.google.com/url?q=http://languagelog.ldc.upenn.edu/nll/?p%3D41594&amp;source=gmail&amp;ust=1549982130644000&amp;usg=AFQjCNGCF-przSJdxkUPiEtaijRXrQPcGw\">\\'Do not accept Taiwan\\'</a>\\xa0\" (2/1/19) &#8212; Conclusion:\\xa0 \" Moral of the story:\\xa0 you are less likely to confuse humans and machines if you write in vernacular.\"</p>'}],\n",
       "   'thr_total': '5'},\n",
       "  {'authors': [{'name': 'Victor Mair',\n",
       "     'href': 'https://www.sas.upenn.edu/ealc/mair'}],\n",
       "   'author_detail': {'name': 'Victor Mair',\n",
       "    'href': 'https://www.sas.upenn.edu/ealc/mair'},\n",
       "   'href': 'https://www.sas.upenn.edu/ealc/mair',\n",
       "   'author': 'Victor Mair',\n",
       "   'title': 'Reclamation of a wasteland by an army unit',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Reclamation of a wasteland by an army unit'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41746'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41746#comments',\n",
       "     'count': '8',\n",
       "     'thr:count': '8'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41746',\n",
       "     'count': '8',\n",
       "     'thr:count': '8'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41746',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41746',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-11T17:10:46Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=11, tm_hour=17, tm_min=10, tm_sec=46, tm_wday=0, tm_yday=42, tm_isdst=0),\n",
       "   'published': '2019-02-11T17:10:46Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=11, tm_hour=17, tm_min=10, tm_sec=46, tm_wday=0, tm_yday=42, tm_isdst=0),\n",
       "   'tags': [{'term': 'Language and food',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Lost in translation',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Topolects',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'Jane Skinner received this from a friend who saw it in Chengdu, Sichuan: The translation for the mysterious fourth item on the menu, \"reclamation of a wasteland by an army unit\" is taken directly from Baidu Fanyi for jūntún 军屯.\\xa0 The translator simply ignored the last two characters, guōkuī 锅盔, which literally mean \"pot helmet\". [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Jane Skinner received this from a friend who saw it in Chengdu, Sichuan: The translation for the mysterious fourth item on the menu, \"reclamation of a wasteland by an army unit\" is taken directly from Baidu Fanyi for jūntún 军屯.\\xa0 The translator simply ignored the last two characters, guōkuī 锅盔, which literally mean \"pot helmet\". [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41746',\n",
       "     'value': '<p>Jane Skinner received this from a friend who saw it in Chengdu, Sichuan:</p>\\n<p align=\"center\"><a href=\"http://languagelog.ldc.upenn.edu/~bgzimmer/reclamation.jpg\"><img title=\"Click to embiggen\" src=\"http://languagelog.ldc.upenn.edu/~bgzimmer/reclamation.jpg\" alt=\"\" width=\"350\" /></a><span id=\"more-41746\"></span></p>\\n<p>The translation for the mysterious fourth item on the menu, \"reclamation of a wasteland by an army unit\" is taken directly from Baidu Fanyi for jūntún 军屯.\\xa0 The translator simply ignored the last two characters, guōkuī 锅盔, which literally mean \"pot helmet\".</p>\\n<p>In actual usage guōkuī 锅盔 is a Shanxi topolectal expression for a type of large, round baked wheat cake:</p>\\n<p style=\"padding-left: 30px;\">Guokui is a kind of pancake made from flour from Shaanxi cuisine. It is round in shape, about a foot long in diameter, an inch in thickness, and weighs about 2.5 kg. It is traditionally presented as a gift by a grandmother to her grandson when he turns one month old.</p>\\n<p style=\"padding-left: 30px;\">(<a href=\"https://en.wikipedia.org/wiki/Guokui\" target=\"_blank\" rel=\"noopener\" data-saferedirecturl=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Guokui&amp;source=gmail&amp;ust=1549982130639000&amp;usg=AFQjCNFuckew5D6yTOx28o4l87zKQCw3xw\">Source</a>)</p>\\n<p>The name is sometimes translated as \"crusty pancake\", though I might suggest \"helmet pancake\" as an alternative.</p>\\n<p>In\\xa0<a href=\"https://www.google.com/search?q=%E9%94%85%E7%9B%94&amp;tbm=isch&amp;source=hp&amp;sa=X&amp;ved=2ahUKEwjZxc6WxbLgAhXSx1kKHbeAAn8Q7Al6BAgDEA0&amp;biw=1440&amp;bih=913\" target=\"_blank\" rel=\"noopener\" data-saferedirecturl=\"https://www.google.com/url?q=https://www.google.com/search?q%3D%25E9%2594%2585%25E7%259B%2594%26tbm%3Disch%26source%3Dhp%26sa%3DX%26ved%3D2ahUKEwjZxc6WxbLgAhXSx1kKHbeAAn8Q7Al6BAgDEA0%26biw%3D1440%26bih%3D913&amp;source=gmail&amp;ust=1549982130639000&amp;usg=AFQjCNFdWbi9_7cUqcF4LsFxq0DqPMP3IQ\">this Google gallery of images</a>, you can see the variety of sizes, shapes, and types that go by the name guōkuī 锅盔 (lit., \"pot helmet\", i.e., \"helmet pancake\").</p>\\n<p>As for jūntún 军屯, it refers to army encampments, so this kind of \"pot helmet pancake\" must evoke memories of being in the field for those who have been soldiers, especially those who served in túntián\\xa0<span class=\"m_4763175428732457621st\"><a href=\"https://en.wikipedia.org/wiki/Tuntian\" target=\"_blank\" rel=\"noopener\" data-saferedirecturl=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Tuntian&amp;source=gmail&amp;ust=1549982130639000&amp;usg=AFQjCNE7r6gov0nBLQ4S85sZGKImulo4Ww\">屯田</a>\\xa0(\"military agro-colonies\") and\\xa0</span><span class=\"m_4763175428732457621st\"><a href=\"https://en.wikipedia.org/wiki/Xinjiang_Production_and_Construction_Corps\" target=\"_blank\" rel=\"noopener\" data-saferedirecturl=\"https://www.google.com/url?q=https://en.wikipedia.org/wiki/Xinjiang_Production_and_Construction_Corps&amp;source=gmail&amp;ust=1549982130639000&amp;usg=AFQjCNHhHhPIYMKlFujFFVRfYHmuRyt0cg\">bīngtuán\\xa0兵团</a>\\xa0(“army unit; army formation”) in border regions such as Xinjiang where vast reclamation projects are undertaken</span>.\\xa0 The bread / pancake certainly looks spartan to me.</p>'}],\n",
       "   'thr_total': '8'},\n",
       "  {'authors': [{'name': 'Victor Mair',\n",
       "     'href': 'https://www.sas.upenn.edu/ealc/mair'}],\n",
       "   'author_detail': {'name': 'Victor Mair',\n",
       "    'href': 'https://www.sas.upenn.edu/ealc/mair'},\n",
       "   'href': 'https://www.sas.upenn.edu/ealc/mair',\n",
       "   'author': 'Victor Mair',\n",
       "   'title': 'The unpredictability of Chinese character formation and pronunciation, pt. 2',\n",
       "   'title_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'The unpredictability of Chinese character formation and pronunciation, pt. 2'},\n",
       "   'links': [{'rel': 'alternate',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41764'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'text/html',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?p=41764#comments',\n",
       "     'count': '20',\n",
       "     'thr:count': '20'},\n",
       "    {'rel': 'replies',\n",
       "     'type': 'application/atom+xml',\n",
       "     'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom&p=41764',\n",
       "     'count': '20',\n",
       "     'thr:count': '20'}],\n",
       "   'link': 'http://languagelog.ldc.upenn.edu/nll/?p=41764',\n",
       "   'id': 'http://languagelog.ldc.upenn.edu/nll/?p=41764',\n",
       "   'guidislink': False,\n",
       "   'updated': '2019-02-11T13:08:16Z',\n",
       "   'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=11, tm_hour=13, tm_min=8, tm_sec=16, tm_wday=0, tm_yday=42, tm_isdst=0),\n",
       "   'published': '2019-02-11T13:08:16Z',\n",
       "   'published_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=11, tm_hour=13, tm_min=8, tm_sec=16, tm_wday=0, tm_yday=42, tm_isdst=0),\n",
       "   'tags': [{'term': 'Errors',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Writing',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None},\n",
       "    {'term': 'Writing systems',\n",
       "     'scheme': 'http://languagelog.ldc.upenn.edu/nll',\n",
       "     'label': None}],\n",
       "   'summary': 'Emma Knightley asks: My background is that I grew up in Taiwan learning Traditional Chinese and now most of what I use in my professional life is in Simplified Chinese. How exactly should the character of hē, \"to drink,\" be written? I grew up learning that the character inside the bottom-right enclosure is 人. Now [&#8230;]',\n",
       "   'summary_detail': {'type': 'text/html',\n",
       "    'language': 'en-US',\n",
       "    'base': 'http://languagelog.ldc.upenn.edu/nll/wp-atom.php',\n",
       "    'value': 'Emma Knightley asks: My background is that I grew up in Taiwan learning Traditional Chinese and now most of what I use in my professional life is in Simplified Chinese. How exactly should the character of hē, \"to drink,\" be written? I grew up learning that the character inside the bottom-right enclosure is 人. Now [&#8230;]'},\n",
       "   'content': [{'type': 'text/html',\n",
       "     'language': 'en-US',\n",
       "     'base': 'http://languagelog.ldc.upenn.edu/nll/?p=41764',\n",
       "     'value': '<p>Emma Knightley asks:</p>\\n<p style=\"padding-left: 30px;\">My background is that I grew up in Taiwan learning Traditional Chinese and now most of what I use in my professional life is in Simplified Chinese. How exactly should the character of hē, \"to drink,\" be written?</p>\\n<p style=\"padding-left: 30px;\">I grew up learning that the character inside the bottom-right enclosure is 人. Now I see that it is mostly written as 匕. I don\\'t know when this changed, and I don\\'t think it\\'s a matter of Traditional vs Simplified, either, as I see both versions in Traditional writing as well. <a href=\"https://zh.wiktionary.org/zh-hans/%E5%96%9D\">This Wiktionary entry illustrates the confusion nicely</a>. No one I know has noticed this change, which leads me to think that I\\'m either losing my mind or experiencing the <a href=\"http://theconversation.com/the-mandela-effect-and-how-your-mind-is-playing-tricks-on-you-89544\">Mandela Effect</a>.</p>\\n<p><span id=\"more-41764\"></span></p>\\n<p>Here\\'s the character that Emma is asking about:\\xa0 喝 (traditional and simplified), but it is also written thus:\\xa0 [Whoops!\\xa0 No matter how hard I try, I can\\'t get my browser to produce this character, even though I use the correct Unicode number, U+FA78.\\xa0 Whoops again!\\xa0 By tricking my browser, I think I can get it to appear here, <img class=\"\" src=\"data:image/png;base64,<html>
<head>
<title>he &quot;drink&quot; kài</title>
<link rel="important stylesheet" href="chrome://messagebody/skin/messageBody.css">
</head>
<body>
<table border=0 cellspacing=0 cellpadding=0 width="100%" class="header-part1"><tr><td><div class="headerdisplayname" style="display:inline;">Subject: </div>he &quot;drink&quot; kài</td></tr><tr><td><div class="headerdisplayname" style="display:inline;">From: </div>Victor Mair &lt;vmair@sas.upenn.edu&gt;</td></tr><tr><td><div class="headerdisplayname" style="display:inline;">Date: </div>2/10/19, 8:28 PM</td></tr></table><table border=0 cellspacing=0 cellpadding=0 width="100%" class="header-part2"><tr><td><div class="headerdisplayname" style="display:inline;">To: </div>&quot;Victor H. Mair&quot; &lt;vmair@sas.upenn.edu&gt;</td></tr></table><br>
<div class="moz-text-html"  lang="x-unicode"><html>
  <head>

    <meta http-equiv="content-type" content="text/html; ">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <p>The unpredictability of Chinese character formation and
      pronunciation, pt. 2<br>
    </p>
    Emma Knightley asks:<br>
    <br>
    =====<br>
    <br>
    <div style="font-family: Calibri, Helvetica, sans-serif; font-size:
      12pt; color: rgb(0, 0, 0);">
      My background is that I grew up in Taiwan learning Traditional
      Chinese and now most of what I use in my professional life is in
      Simplified Chinese. How exactly should the character of hē, "to
      drink," be written?</div>
    <div style="font-family: Calibri, Helvetica, sans-serif; font-size:
      12pt; color: rgb(0, 0, 0);">
      <br>
    </div>
    <div style="font-family: Calibri, Helvetica, sans-serif; font-size:
      12pt; color: rgb(0, 0, 0);">
      I grew up learning that the character inside the bottom-right
      enclosure is 人. Now I see that it is mostly written as 匕. I don't
      know when this changed, and I don't think it's a matter of
      Traditional vs Simplified, either, as I see both versions in
      Traditional writing as well. <a
        href="https://zh.wiktionary.org/zh-hans/%E5%96%9D"
        title="https://zh.wiktionary.org/zh-hans/%E5%96%9D">
        This Wiktionary entry illustrates the confusion nicely</a>. No
      one I know has noticed this change, which leads me to think that
      I'm either losing my mind or experiencing the
      <a
href="http://theconversation.com/the-mandela-effect-and-how-your-mind-is-playing-tricks-on-you-89544"
title="http://theconversation.com/the-mandela-effect-and-how-your-mind-is-playing-tricks-on-you-89544">
        Mandela Effect</a>.</div>
    <br>
    =====<br>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <br>
    ===================================<br>
    <br>
    Here's the character that Emma is asking about:  喝 (traditional and
    simplified), but it is also written thus:  [Whoops!  No matter how
    hard I try, I can't get my browser to produce this character, even
    though I use the correct Unicode number, U+FA78.  Whoops again!  By
    tricking my browser, I think I can get it to appear here, <img
      src="imap://vmair%40upenn%2Eedu@outlook.office365.com:993/fetch%3EUID%3E/Drafts%3E96903?24-559D&part=1.2" align="middle">,
    though I can't guarantee it'll still be there when I make this post
    (I won't attempt to type it again later in this post, but will
    simply refer to it as "the phantom character").  It's the form with
    人 in the bottom-right enclosure, not 匕.  Actually, in the latter
    version, the stroke that starts at the top left, goes straight down
    vertically, then abruptly curves to the right before ending with an
    upward hook serves as the left side and bottom of the bottom right
    of this form of hē ("to drink"), so the only thing inside the
    "enclosure" at the bottom right is a short stroke that slants
    downward to the left, not 匕.<br>
    <br>
    The Wiktionary entry for hē ("to drink") does indeed illustrate the
    problem nicely, since the character in the heading is the one my
    browser won't produce, but the examples for Chinese, Japanese, and
    Korean all have 喝, while the one for Vietnamese has the one that I
    can't call up with U+FA78.<br>
    <br>
    As for the pronunciation of 喝 and the phantom character I can't
    produce, we have:<br>
    <br>
    =====<br>
    <br>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <span class="dicpy">hē ("to drink")<br>
    </span><br>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <span class="dicpy">hè ("shout loudly")<br>
    </span><br>
    <span class="dicpy">
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
      <span class="dicpy">yè ("with a hoarse sound")<br>
        <br>
        kài (listed in Wiktionary and other sources, but I haven't been
        able to track it down otherwise and assign a meaning to it, so
        I'm a bit dubious about its existence as a morphosyllable in
        MSM, though it may well exist in one or another topolect [e.g.,
        Taiwanese], though I'm not sure where it comes from and I have
        no idea what it means)</span></span><br>
    <br>
    <a
href="https://www.mdbg.net/chinese/dictionary?page=worddict&amp;wdrst=0&amp;wdqb=%E5%96%9D#"
      title="Show information about all characters"
      onclick="aj('fd51bd',this,'cdqchi',2,'欱');
      trackExitLink('inline-cdqchi', 'result-wdb'); return false"><span
        class="mpt1"></span></a> Aside from the phantom character that I
    can't type, variants of 喝 include 欱 and 哈, and there are others that
    I cannot type.  The main point of this post, however, is that Emma
    and I, and doubtless millions of others, were not too long ago
    taught to write hē ("to drink") with a form of the character
    (U+FA78) that now barely exists.  If we wrote 喝, it would have been
    considered an error.<br>
    <br>
    <b>Readings</b><br>
    <br>
    "<a href="http://languagelog.ldc.upenn.edu/nll/?p=3750"
      rel="bookmark" title="Permanent link to The unpredictability of
      Chinese character formation and pronunciation">The
      unpredictability of Chinese character formation and pronunciation</a>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    " (2/6/12)<br>
    <br>
    <br>
    <br>
    <pre class="moz-signature" cols="72">-- 

Victor H. Mair, Professor of Chinese Language and Literature
Department of East Asian Languages and Civilizations
847/9 Williams Hall
University of Pennsylvania
255 South 36th St.
Philadelphia, PA 19104-6305
USA

Tel.:  215-898-8432 (**please leave a message!**)
Dept. office:  215-898-7466 (for messages and inquiries)
Fax:  215-573-9617 (mark for the attention of Prof. Victor H. Mair)
<a class="moz-txt-link-abbreviated" href="mailto:e-mail:vmair@sas.upenn.edu">e-mail:vmair@sas.upenn.edu</a> (read infrequently)

ALWAYS CHECK YOUR SPAM / JUNK FILES / FOLDERS!!  
MUCH LEGITIMATE E-MAIL ENDS UP THERE AND GETS LOST UNLESS YOU RETRIEVE IT.

For a complete catalog of SINO-PLATONIC PAPERS (SPP) and information about ordering, go to: 
<a class="moz-txt-link-abbreviated" href="http://www.sino-platonic.org">www.sino-platonic.org</a>     New issues published regularly on the Web; all back issues still in print.

LANGUAGE LOG:  <a class="moz-txt-link-freetext" href="http://languagelog.ldc.upenn.edu/nll/">http://languagelog.ldc.upenn.edu/nll/</a></pre>
  </body>
</html>

</div></body>
</html>
</table></div>\" align=\"middle\" />, though I can\\'t guarantee it\\'ll still be there when I make this post (I won\\'t attempt to type it again later in this post, but will simply refer to it as \"the phantom character\")].\\xa0 It\\'s the form with 人 in the bottom-right enclosure, not 匕.\\xa0 Actually, in the latter version, the stroke that starts at the top left, goes straight down vertically, then abruptly curves to the right before ending with an upward hook serves as the left side and bottom of the bottom right of this form of hē (\"to drink\"), so the only thing inside the \"enclosure\" at the bottom right is a short stroke that slants downward to the left, not 匕.</p>\\n<p>The Wiktionary entry for hē (\"to drink\") does indeed illustrate the problem nicely, since the character in the heading is the one my browser won\\'t produce, but the examples for Chinese, Japanese, and Korean all have 喝, while the one for Vietnamese has the one that I can\\'t call up with U+FA78.</p>\\n<p>As for the pronunciation of 喝 and the phantom character I can\\'t produce, we have:</p>\\n<p style=\"padding-left: 30px;\"><span class=\"dicpy\">hē (\"to drink\")<br />\\n</span><br />\\n<span class=\"dicpy\">hè (\"shout loudly\")<br />\\n</span><br />\\n<span class=\"dicpy\"> yè (\"with a hoarse sound\")</span></p>\\n<p style=\"padding-left: 30px;\">kài (listed in Wiktionary and other sources, but I haven\\'t been able to track it down otherwise and assign a meaning to it, so I\\'m a bit dubious about its existence as a morphosyllable in MSM, though it may well exist in one or another topolect [e.g., Taiwanese], though I\\'m not sure where it comes from and I have no idea what it means) &#8212; FLASH!!\\xa0 Just found it in Hànyǔ dà zìdiǎn 漢語大字典 (Unabridged character dictionary of Sinitic; HDZ), 1.653b, where it says that, in this reading, 喝 = alas, can\\'t copy this one either; it is the fourth rare variant from the left under yìtǐzì 异体字 &#8212; top center <a href=\"http://www.zdic.net/z/16/js/559D.htm\">here</a>\\xa0(HDZ says, believe it or not, that it means \"sound\" [shēng 聲])</p>\\n<p>Aside from the phantom character that I can\\'t type, variants of 喝 include 欱 and 哈, and there are about half a dozen others that I cannot type.\\xa0 The main point of this post, however, is that Emma and I, and doubtless millions of others, were not too long ago taught to write hē (\"to drink\") with a form of the character (U+FA78) that now barely exists.\\xa0 If we wrote 喝, it would have been considered an error.</p>\\n<p><b>Readings</b></p>\\n<ul>\\n<li>\"<a title=\"Permanent link to The unpredictability of Chinese character formation and pronunciation\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=3750\" rel=\"bookmark\">The unpredictability of Chinese character formation and pronunciation</a>\" (2/6/12)</li>\\n<li>\"<a title=\"Permanent link to How many more Chinese characters are needed?\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=29034\" rel=\"bookmark\">How many more Chinese characters are needed?</a>\" (10/25/16)</li>\\n<li>\"<a title=\"Permanent link to Chinese character inputting\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=21729\" rel=\"bookmark\">Chinese character inputting</a>\" (10/17/15)</li>\\n<li>\"<a title=\"Permanent link to Is there a practical limit to how much can fit in Unicode?\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=35136\" rel=\"bookmark\">Is there a practical limit to how much can fit in Unicode?</a>\" (10/27/17)</li>\\n<li>\"<a title=\"Permanent link to Character crises\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=38784\" rel=\"bookmark\">Character crises</a>\" (6/15/18)</li>\\n<li>\"<a title=\"Permanent link to Ask Language Log: Looking up hanzi for ignoramuses\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=35585\" rel=\"bookmark\">Ask Language Log: Looking up hanzi for ignoramuses</a>\" (11/29/17)</li>\\n<li>\"<a title=\"Permanent link to Sinological suffering\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=31859\" rel=\"bookmark\">Sinological suffering</a>\" (3/31/17)</li>\\n<li>\"<a title=\"Permanent link to Writing characters and writing letters\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=40604\" rel=\"bookmark\">Writing characters and writing letters</a>\" (11/17/18)</li>\\n<li>\"<a title=\"Permanent link to An immodest proposal: \" href=\"http://languagelog.ldc.upenn.edu/nll/?p=40730\" rel=\"bookmark\">An immodest proposal: \\'Boycott the Chinese Language\\'</a>\" (11/18/18)</li>\\n<li>\"<a title=\"Permanent link to The wrong way to write Chinese characters\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=40819\" rel=\"bookmark\">The wrong way to write Chinese characters</a>\" (11/28/18)</li>\\n<li>\"<a title=\"Permanent link to Sinographs by the numbers\" href=\"http://languagelog.ldc.upenn.edu/nll/?p=41496\" rel=\"bookmark\">Sinographs by the numbers</a>\" (1/22/19)</li>\\n</ul>'}],\n",
       "   'thr_total': '20'}],\n",
       " 'bozo': 0,\n",
       " 'headers': {'Date': 'Sun, 17 Feb 2019 15:28:41 GMT',\n",
       "  'Server': 'Apache/2.2.16 (Debian)',\n",
       "  'X-Powered-By': 'PHP/5.3.3-7+squeeze25',\n",
       "  'Last-Modified': 'Sun, 17 Feb 2019 15:03:09 GMT',\n",
       "  'ETag': '\"8d63c65a392ba22bcf9efcbc751282a0\"',\n",
       "  'Link': '<http://languagelog.ldc.upenn.edu/nll/index.php?rest_route=/>; rel=\"https://api.w.org/\"',\n",
       "  'Connection': 'close',\n",
       "  'Transfer-Encoding': 'chunked',\n",
       "  'Content-Type': 'application/atom+xml; charset=UTF-8'},\n",
       " 'etag': '\"8d63c65a392ba22bcf9efcbc751282a0\"',\n",
       " 'updated': 'Sun, 17 Feb 2019 15:03:09 GMT',\n",
       " 'updated_parsed': time.struct_time(tm_year=2019, tm_mon=2, tm_mday=17, tm_hour=15, tm_min=3, tm_sec=9, tm_wday=6, tm_yday=48, tm_isdst=0),\n",
       " 'href': 'http://languagelog.ldc.upenn.edu/nll/?feed=atom',\n",
       " 'status': 200,\n",
       " 'encoding': 'UTF-8',\n",
       " 'version': 'atom10',\n",
       " 'namespaces': {'': 'http://www.w3.org/2005/Atom',\n",
       "  'thr': 'http://purl.org/syndication/thread/1.0',\n",
       "  'georss': 'http://www.georss.org/georss',\n",
       "  'geo': 'http://www.w3.org/2003/01/geo/wgs84_pos#'}}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "feed=feedparser.parse('http://languagelog.ldc.upenn.edu/nll/?feed=atom')\n",
    "feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextFeed(feed):\n",
    "    entries=[]\n",
    "    for e in feed.entries:\n",
    "        if len(re.findall(r'as best as (?:I|you|he|she|it|we|they|what|who)* can',e['content'][0]['value']))!=0:            \n",
    "            print(e['title'])\n",
    "            entries.append(e)\n",
    "    return entries;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTextFeed(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "def getText(text):\n",
    "    return re.findall(r'as best [as] (?:I|you|he|she|it|we|they|what|who)* can|as best (?:I|you|he|she|it|we|they|what|who)* can',text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as best you can']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getText(webtext.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 36\n",
    "Study the lolcat version of the book of Genesis, accessible as nltk.corpus.genesis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expressions to convert English words into corresponding lolspeak words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oh', 'hai', '.', 'In', 'teh', 'beginnin', 'Ceiling', ...]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lolcat=nltk.corpus.genesis.words('lolcat.txt')\n",
    "lolcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 37\n",
    "Read about the re.sub() function for string substitution using regular expressions, using help(re.sub) and by consulting the further readings for this chapter. Use re.sub in writing code to remove HTML tags from an HTML file, and to normalize whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to speak lolcat - LOLCat Bible Translation Project\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "response=request.urlopen('http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat')\n",
    "raw = response.read().decode('utf8')\n",
    "html=BeautifulSoup(raw).get_text()\n",
    "print(html[2:56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to say lolcat - LOLCat Bible Translation Project\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('speak','say',html[2:56]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 38\n",
    "An interesting challenge for tokenization is words that have been split across a line-break. E.g. if long-term is split, then we have the string long-\\nterm.<br>\n",
    "\n",
    "Write a regular expression that identifies words that are hyphenated at a line-break. The expression will need to include the \\n character.\n",
    "Use re.sub() to remove the \\n character from these words.<br>\n",
    "How might you identify words that should not remain hyphenated once the newline is removed, e.g. 'encyclo-\\npedia'?x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['long-\\nterm', 'short-\\nterm']\n"
     ]
    }
   ],
   "source": [
    "words=re.findall('\\w+-\\n\\w+','It is both a long-\\nterm goal and short-\\nterm goal')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long-term', 'short-term']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\n','',' '.join(words)).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "words+=['encycl-\\nopedia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long-term\n",
      "short-term\n",
      "encyclopedia\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    word = re.sub('\\n', '', w)\n",
    "    parts = word.lower().split('-')\n",
    "    if (parts[0] not in nltk.corpus.words.words() and parts[1] not in nltk.corpus.words.words()):\n",
    "        print(re.sub('\\-', '', word))\n",
    "    else:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 39\n",
    "Read the Wikipedia entry on Soundex. Implement this algorithm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 40\n",
    "Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g. compare ABC Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 41\n",
    "Rewrite the following nested loop as a nested list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution','sequoia', 'tenacious', 'unidirectional']\n",
    "vsequences = set()\n",
    "for word in words:\n",
    "    vowels = []\n",
    "    for char in word:\n",
    "        if char in 'aeiou':\n",
    "            vowels.append(char)\n",
    "    vsequences.add(''.join(vowels))\n",
    "sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsequences = set([''.join([char for char in word if char in 'aeiou']) for word in words])\n",
    "sorted(vsequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 42\n",
    "Use WordNet to create a semantic index for a text collection. Extend the concordance search program in 3.6, indexing each word using the offset of its first synset, e.g. wn.synsets('dog')[0].offset (and optionally the offset of some of its ancestors in the hypernym hierarchy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 43\n",
    "With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (nltk.corpus.udhr), and NLTK's frequency distribution and rank correlation functionality (nltk.FreqDist,  nltk.spearman_correlation), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 44\n",
    "Write a program that processes a text and discovers cases where a word has been used with a novel sense. For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context. (Note that this is a crude approach; doing it well is a difficult, open research problem.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 45\n",
    "Read the article on normalization of non-standard words (Sproat et al, 2001), and implement a similar system for text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
